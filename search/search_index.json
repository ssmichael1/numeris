{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"numeris","text":"<p>A mostly self-contained numerical algorithms library with minimal external dependencies.</p> <p>numeris is suitable for use on embedded microprocessors with no operating system or heap allocation (<code>no-std</code>), while still taking advantage of SIMD instructions to be highly performant on more capable hardware. The same code compiles and runs correctly in both environments.</p> <p>Alpha software</p> <p>APIs are unstable and may change without notice before 1.0.</p>"},{"location":"#feature-highlights","title":"Feature Highlights","text":"Area What's included Matrix Stack-allocated <code>Matrix&lt;T, M, N&gt;</code> with const-generic dimensions; <code>DynMatrix&lt;T&gt;</code> for runtime sizes Linear Algebra LU, Cholesky, QR, SVD, symmetric eigendecomposition, real Schur decomposition ODE Fixed-step RK4 + 7 adaptive Runge-Kutta solvers + RODAS4 stiff solver Optimization Brent, Newton, BFGS, Gauss-Newton, Levenberg-Marquardt State Estimation EKF, UKF, SR-UKF, CKF, RTS smoother, batch least-squares Interpolation Linear, Hermite, Lagrange, cubic spline, bilinear Special Functions gamma, lgamma, digamma, beta, betainc, incomplete gamma, erf Statistics 10 distributions (Normal, Gamma, Beta, Student's t, Poisson, \u2026) Digital Control Butterworth/Chebyshev IIR filters, PID controller Quaternion Unit quaternion rotations, SLERP, Euler angles, rotation matrices SIMD NEON (aarch64), SSE2/AVX/AVX-512 (x86_64) \u2014 always-on, no feature flag"},{"location":"#quick-start","title":"Quick Start","text":"<p>Add to <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\nnumeris = \"0.2\"\n</code></pre> <pre><code>use numeris::{Matrix, Vector};\n\n// Solve Ax = b\nlet a = Matrix::new([\n    [2.0_f64, 1.0, -1.0],\n    [-3.0, -1.0,  2.0],\n    [-2.0,  1.0,  2.0],\n]);\nlet b = Vector::from_array([8.0, -11.0, -3.0]);\nlet x = a.solve(&amp;b).unwrap(); // x = [2, 3, -1]\n\n// Cholesky decomposition\nlet spd = Matrix::new([[4.0_f64, 2.0], [2.0, 3.0]]);\nlet inv = spd.cholesky().unwrap().inverse();\n\n// Symmetric eigendecomposition\nlet sym = Matrix::new([[4.0_f64, 1.0], [1.0, 3.0]]);\nlet eig = sym.eig_symmetric().unwrap();\nlet vals = eig.eigenvalues();   // sorted ascending\nlet vecs = eig.eigenvectors();  // columns = eigenvectors\n\n// Quaternion rotation\nuse numeris::Quaternion;\nlet q = Quaternion::from_axis_angle(\n    &amp;Vector::from_array([0.0, 0.0, 1.0]),\n    std::f64::consts::FRAC_PI_2,\n);\nlet v = Vector::from_array([1.0, 0.0, 0.0]);\nlet rotated = q * v; // \u2248 [0, 1, 0]\n</code></pre>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li>Getting Started \u2014 installation, feature flags, first examples</li> <li>Module docs \u2014 detailed pages for each module (use the top tabs)</li> <li>Performance \u2014 SIMD tiers, benchmark numbers vs. nalgebra and faer</li> <li>No-std / Embedded \u2014 usage without std or heap allocation</li> <li>Design \u2014 architectural decisions and trait hierarchy</li> <li>API Reference \u2197 \u2014 full rustdoc on docs.rs</li> </ul>"},{"location":"control/","title":"Digital Control","text":"<p>Biquad-cascade IIR filters and a discrete-time PID controller. All components are no-std compatible, use no complex arithmetic at runtime, and work with both <code>f32</code> and <code>f64</code>.</p> <p>Requires the <code>control</code> Cargo feature:</p> <pre><code>numeris = { version = \"0.2\", features = [\"control\"] }\n</code></pre>"},{"location":"control/#iir-filter-design","title":"IIR Filter Design","text":"<p>numeris designs IIR filters by:</p> <ol> <li>Computing analog prototype poles (Butterworth or Chebyshev Type I)</li> <li>Applying a bilinear transform (Tustin's method) with frequency pre-warping</li> <li>Pairing poles into second-order sections (biquads)</li> </ol>"},{"location":"control/#butterworth","title":"Butterworth","text":"<p>Maximally flat passband, monotone rolloff.</p> <pre><code>use numeris::control::{butterworth_lowpass, butterworth_highpass, BiquadCascade};\n\n// 4th-order lowpass at 1 kHz, 8 kHz sample rate\n// \u2192 2 biquad sections (N/2 = 2 for even order)\nlet mut lpf: BiquadCascade&lt;f64, 2&gt; = butterworth_lowpass(4, 1000.0, 8000.0).unwrap();\n\n// Process one sample\nlet y = lpf.tick(1.0);\n\n// Process a block\nlet input  = [1.0_f64, 0.5, -0.3, 0.8, 0.2];\nlet mut output = [0.0_f64; 5];\nlpf.reset();\nlpf.process(&amp;input, &amp;mut output);\n\n// In-place processing\nlet mut buf = [1.0_f64, 0.5, -0.3];\nlpf.reset();\nlpf.process_inplace(&amp;mut buf);\n\n// Highpass version\nlet mut hpf: BiquadCascade&lt;f64, 2&gt; = butterworth_highpass(4, 2000.0, 8000.0).unwrap();\n</code></pre>"},{"location":"control/#chebyshev-type-i","title":"Chebyshev Type I","text":"<p>Steeper rolloff than Butterworth at the cost of passband ripple.</p> <pre><code>use numeris::control::{chebyshev1_lowpass, chebyshev1_highpass, BiquadCascade};\n\n// 4th-order Chebyshev lowpass, 1 dB passband ripple\nlet mut cheb: BiquadCascade&lt;f64, 2&gt; = chebyshev1_lowpass(4, 1.0, 1000.0, 8000.0).unwrap();\n\nlet y = cheb.tick(1.0);\n\n// Highpass version\nlet mut cheb_hp: BiquadCascade&lt;f64, 2&gt; = chebyshev1_highpass(4, 1.0, 2000.0, 8000.0).unwrap();\n</code></pre>"},{"location":"control/#design-function-signatures","title":"Design Function Signatures","text":"<pre><code>// Butterworth\nfn butterworth_lowpass&lt;T, const N: usize&gt;(\n    order: usize,           // filter order (1..=2N, even fills N sections, odd uses degenerate last)\n    cutoff_hz: T,           // -3 dB frequency\n    sample_hz: T,           // sample rate\n) -&gt; Result&lt;BiquadCascade&lt;T, N&gt;, ControlError&gt;\n\nfn butterworth_highpass&lt;T, const N: usize&gt;(\n    order: usize,\n    cutoff_hz: T,\n    sample_hz: T,\n) -&gt; Result&lt;BiquadCascade&lt;T, N&gt;, ControlError&gt;\n\n// Chebyshev Type I\nfn chebyshev1_lowpass&lt;T, const N: usize&gt;(\n    order: usize,\n    ripple_db: T,           // passband ripple in dB (&gt; 0)\n    cutoff_hz: T,\n    sample_hz: T,\n) -&gt; Result&lt;BiquadCascade&lt;T, N&gt;, ControlError&gt;\n\nfn chebyshev1_highpass&lt;T, const N: usize&gt;(\n    order: usize,\n    ripple_db: T,\n    cutoff_hz: T,\n    sample_hz: T,\n) -&gt; Result&lt;BiquadCascade&lt;T, N&gt;, ControlError&gt;\n</code></pre>"},{"location":"control/#comparison","title":"Comparison","text":"Design Passband Stopband Notes Butterworth Flat Monotone General purpose Chebyshev Type I Ripple \u2264 N dB Steeper Better rolloff, slight passband distortion"},{"location":"control/#biquad-sections","title":"Biquad Sections","text":"<p>A <code>Biquad&lt;T&gt;</code> implements a single second-order IIR section in Direct Form II Transposed (DFII-T), which is numerically better conditioned than Direct Form I:</p> <pre><code>y[n] = b0*x[n] + s1[n-1]\ns1[n] = b1*x[n] - a1*y[n] + s2[n-1]\ns2[n] = b2*x[n] - a2*y[n]\n</code></pre> <p>Coefficients follow the convention <code>H(z) = (b0 + b1 z\u207b\u00b9 + b2 z\u207b\u00b2) / (1 + a1 z\u207b\u00b9 + a2 z\u207b\u00b2)</code>.</p> <pre><code>use numeris::control::Biquad;\n\n// Construct manually (e.g., from external design tool)\nlet bq = Biquad::new(\n    1.0_f64, 2.0, 1.0,   // b0, b1, b2\n    1.0, -1.8, 0.81,     // a0 (must be 1.0), a1, a2\n);\n\nlet mut state = bq.initial_state();\nlet y = bq.tick(1.0, &amp;mut state);\n</code></pre> <p><code>BiquadCascade&lt;T, N&gt;</code> chains <code>N</code> biquad sections in series.</p>"},{"location":"control/#pid-controller","title":"PID Controller","text":"<p>Discrete-time PID with:</p> <ul> <li>Trapezoidal integration (bilinear, no integrator windup from step changes)</li> <li>Derivative on measurement (avoids derivative kick on setpoint changes)</li> <li>Optional derivative LPF (reduces noise amplification)</li> <li>Anti-windup via back-calculation (clamps integrator when output saturates)</li> <li>Output limits (hard clamp)</li> </ul> <pre><code>use numeris::control::Pid;\n\n// Kp=2.0, Ki=0.5, Kd=0.1, sample period dt=0.001 s (1 kHz)\nlet mut pid = Pid::new(2.0_f64, 0.5, 0.1, 0.001)\n    .with_output_limits(-10.0, 10.0)           // clamp output to \u00b110\n    .with_derivative_filter(0.01);              // LPF time constant \u03c4=10ms\n\nlet setpoint    = 1.0_f64;\nlet mut process = 0.0_f64;\n\nfor _ in 0..1000 {\n    let u = pid.tick(setpoint, process);\n    // Simple first-order plant: \u03c4_plant = 0.1 s\n    process += 0.001 * (-process + u);\n}\n\nassert!((process - setpoint).abs() &lt; 0.01);\n</code></pre>"},{"location":"control/#api","title":"API","text":"<pre><code>// Constructor\nfn Pid::new(kp: T, ki: T, kd: T, dt: T) -&gt; Pid&lt;T&gt;\n\n// Builders\nfn with_output_limits(self, min: T, max: T) -&gt; Pid&lt;T&gt;\nfn with_derivative_filter(self, tau: T)     -&gt; Pid&lt;T&gt;   // \u03c4 = filter time constant\n\n// Runtime\nfn tick(&amp;mut self, setpoint: T, measurement: T) -&gt; T   // compute output u[k]\nfn reset(&amp;mut self)                                      // clear integrator and derivative state\n</code></pre>"},{"location":"control/#difference-equations","title":"Difference Equations","text":"<p>Let <code>e[k] = setpoint[k] - measurement[k]</code>. With trapezoidal integration and derivative on measurement:</p> <pre><code>I[k] = I[k-1] + Ki * dt/2 * (e[k] + e[k-1])     (bilinear integration)\nD[k] = -Kd/\u03c4 * (measurement[k] - measurement[k-1]) / dt + (1 - dt/\u03c4) * D[k-1]  (filtered derivative)\nu[k] = clamp(Kp*e[k] + I[k] + D[k])\n</code></pre> <p>Anti-windup: if <code>u</code> is clamped, the integrator is back-corrected: <code>I[k] -= Kb * (u_unclamped - u_clamped)</code>.</p>"},{"location":"control/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::control::ControlError;\n\nmatch butterworth_lowpass::&lt;f64, 2&gt;(4, 1000.0, 8000.0) {\n    Ok(filter) =&gt; { /* use filter */ }\n    Err(ControlError::InvalidOrder)     =&gt; { /* order too large for N sections */ }\n    Err(ControlError::InvalidFrequency) =&gt; { /* cutoff &gt;= nyquist */ }\n    Err(ControlError::InvalidRipple)    =&gt; { /* ripple_db &lt;= 0 */ }\n}\n</code></pre>"},{"location":"design/","title":"Design","text":"<p>Key architectural decisions in numeris.</p>"},{"location":"design/#column-major-storage","title":"Column-Major Storage","text":"<p><code>Matrix&lt;T, M, N&gt;</code> stores data as <code>[[T; M]; N]</code> \u2014 N columns of M rows. Element <code>(row, col)</code> is at <code>data[col][row]</code>.</p> <p>This is column-major (Fortran / LAPACK order). The reasons:</p> <ol> <li>LAPACK compatibility: all standard numerical algorithms (LU, QR, Cholesky, etc.) are formulated and optimized for column-major storage.</li> <li>SIMD inner loops: Householder reflections, LU elimination, and BLAS-1 AXPY (<code>y += \u03b1\u00b7x</code>) operate on a single column \u2014 a contiguous block of M floats. <code>col_as_slice()</code> returns this directly, enabling SIMD dispatch with no gathering.</li> <li>Matmul: the j-k-i loop order accesses A in column-major (contiguous i-access) and B by row (B[k,j] strides), matching the memory layout for the SIMD-vectorized i-loop.</li> </ol> <p><code>Matrix::new()</code> accepts row-major input (as written on paper) and transposes internally, so the API is natural. <code>DynMatrix::from_rows()</code> does the same.</p>"},{"location":"design/#stack-allocation-and-const-generics","title":"Stack Allocation and Const Generics","text":"<p>Fixed-size matrices are fully stack-allocated. There is no heap usage, no <code>Box</code>, no <code>Vec</code> \u2014 just <code>[[T; M]; N]</code> on the stack. This enables:</p> <ul> <li>No-std / embedded: works on targets with no heap allocator.</li> <li>Zero-cost abstractions: the compiler can see all dimensions at monomorphization time. Dead branches are eliminated. SIMD kernel dispatch via <code>TypeId</code> is a compile-time decision.</li> <li>LLVM optimization: the compiler knows the loop bounds and can unroll, vectorize, and schedule optimally.</li> </ul> <p>Avoids <code>[T; M*N]</code> flat storage (which would require unstable <code>generic_const_exprs</code> for expressions like <code>const LEN: usize = M * N</code>).</p>"},{"location":"design/#matrixref-matrixmut-traits","title":"MatrixRef / MatrixMut Traits","text":"<p>All decomposition free functions are written against two simple accessor traits:</p> <pre><code>pub trait MatrixRef&lt;T&gt; {\n    fn nrows(&amp;self) -&gt; usize;\n    fn ncols(&amp;self) -&gt; usize;\n    fn get(&amp;self, row: usize, col: usize) -&gt; T;\n    fn col_as_slice(&amp;self, col: usize) -&gt; &amp;[T];\n}\n\npub trait MatrixMut&lt;T&gt;: MatrixRef&lt;T&gt; {\n    fn get_mut(&amp;mut self, row: usize, col: usize) -&gt; &amp;mut T;\n    fn col_as_mut_slice(&amp;mut self, col: usize) -&gt; &amp;mut [T];\n}\n</code></pre> <p>Both <code>Matrix&lt;T, M, N&gt;</code> and <code>DynMatrix&lt;T&gt;</code> implement these traits. The consequence: the same LU, Cholesky, QR, SVD, Eigen, and Schur code handles fixed and dynamic matrices. There are no separate implementations \u2014 just one set of free functions.</p> <p><code>col_as_slice()</code> / <code>col_as_mut_slice()</code> are the key methods: they return contiguous <code>&amp;[T]</code> slices of individual columns, enabling SIMD dispatch to operate on contiguous data without gathering.</p>"},{"location":"design/#linalgscalar-hierarchy","title":"LinalgScalar Hierarchy","text":"<p>Three element traits form a hierarchy:</p> <pre><code>Scalar          Copy + PartialEq + Debug + Zero + One + Num\n  \u2514\u2500 FloatScalar    Scalar + Float + LinalgScalar&lt;Real=Self&gt;\n  \u2514\u2500 LinalgScalar   Scalar + modulus + conj + re + lsqrt + lln + from_real\n</code></pre> Trait Used by Examples <code>Scalar</code> Matrix ops, arithmetic, iteration <code>i32</code>, <code>u64</code>, <code>f32</code>, <code>f64</code>, <code>Complex&lt;f64&gt;</code> <code>FloatScalar</code> Quaternion, ODE, optim, estimate, ordered comparisons <code>f32</code>, <code>f64</code> <code>LinalgScalar</code> Decompositions, norms <code>f32</code>, <code>f64</code>, <code>Complex&lt;f32&gt;</code>, <code>Complex&lt;f64&gt;</code> <p>Integer matrices work: <code>Matrix&lt;i32, 3, 3&gt;</code> supports arithmetic, indexing, iteration, and transpose. It does not support <code>det()</code>, norms, or decompositions (those require <code>LinalgScalar</code>).</p> <p>Complex is additive: <code>LinalgScalar</code> methods on <code>f64</code> \u2014 <code>conj()</code>, <code>re()</code>, <code>modulus()</code> \u2014 are <code>#[inline]</code> identity functions, fully erased by the compiler. Complex support adds zero code to real-valued paths.</p>"},{"location":"design/#in-place-algorithms","title":"In-Place Algorithms","text":"<p>Decompositions operate in-place on <code>&amp;mut impl MatrixMut&lt;T&gt;</code>. This avoids:</p> <ul> <li>Allocator/storage traits (nalgebra's approach)</li> <li>Cloning the input (scipy's approach)</li> <li>Extra indirection through trait objects</li> </ul> <p>The pattern:</p> <pre><code>pub fn cholesky_in_place&lt;T: LinalgScalar&gt;(\n    a: &amp;mut impl MatrixMut&lt;T&gt;,\n) -&gt; Result&lt;(), LinalgError&gt; {\n    // modifies a in-place: a \u2192 L (lower triangular factor)\n}\n</code></pre> <p>Wrapper structs (<code>CholeskyDecomposition</code>, etc.) own the modified matrix and provide <code>solve()</code>, <code>inverse()</code>, <code>det()</code> as convenience methods.</p>"},{"location":"design/#simd-dispatch","title":"SIMD Dispatch","text":"<p>SIMD is selected at monomorphization time via <code>TypeId</code>:</p> <pre><code>pub fn dot_dispatch&lt;T: Scalar&gt;(a: &amp;[T], b: &amp;[T]) -&gt; T {\n    use core::any::TypeId;\n    if TypeId::of::&lt;T&gt;() == TypeId::of::&lt;f64&gt;() {\n        // SAFETY: we just verified T = f64\n        unsafe { f64_dot(transmute(a), transmute(b)) as T }\n    } else if TypeId::of::&lt;T&gt;() == TypeId::of::&lt;f32&gt;() {\n        unsafe { f32_dot(transmute(a), transmute(b)) as T }\n    } else {\n        scalar_dot(a, b)  // generic fallback for integers, complex, etc.\n    }\n}\n</code></pre> <p>The <code>TypeId</code> check is a compile-time constant \u2014 dead branches are eliminated by LLVM at monomorphization. For integer <code>T</code>, the entire SIMD path compiles away; for <code>f64</code>, only the f64 path remains.</p> <p>The <code>Scalar</code> trait has a <code>'static</code> bound (required by <code>TypeId</code>). This is backwards-compatible \u2014 all scalar types are <code>'static</code>.</p>"},{"location":"design/#dynmatrix-storage","title":"DynMatrix Storage","text":"<p><code>DynMatrix&lt;T&gt;</code> uses <code>Vec&lt;T&gt;</code> in column-major order: element <code>(row, col)</code> is at index <code>col * nrows + row</code>. Implementing <code>MatrixRef</code>/<code>MatrixMut</code> means all decomposition free functions work automatically, with no duplicate code.</p> <p><code>DynVector</code> is a newtype wrapper around <code>DynMatrix</code> that enforces the single-column invariant and provides single-index access.</p>"},{"location":"design/#avoiding-unstable-features","title":"Avoiding Unstable Features","text":"<p>numeris uses only stable Rust (MSRV 1.70). Constraints:</p> <ul> <li><code>generic_const_exprs</code> is unstable \u2192 no <code>[T; M*N]</code> flat storage</li> <li><code>min_const_generics</code> (stable since 1.51) \u2192 <code>[[T; M]; N]</code> two-level storage works</li> <li><code>core::arch</code> intrinsics for SIMD are stable on aarch64 and x86_64</li> </ul>"},{"location":"design/#num-traits-integration","title":"num-traits Integration","text":"<p>numeris uses <code>num-traits</code> with <code>default-features = false</code> for generic numeric bounds. This avoids pulling in <code>std</code> through <code>num-traits</code> in no-std mode. Key traits used:</p> <ul> <li><code>Zero</code>, <code>One</code> \u2014 additive/multiplicative identity</li> <li><code>Num</code> \u2014 basic arithmetic (Scalar bound)</li> <li><code>Float</code> \u2014 transcendentals, <code>sqrt</code>, <code>sin</code>, etc. (FloatScalar bound)</li> </ul> <p>When <code>std</code> is enabled, <code>Float</code> delegates to the system's hardware libm. Without <code>std</code>, it uses <code>libm</code>'s software implementations via the <code>libm</code> feature.</p>"},{"location":"dynmatrix/","title":"DynMatrix","text":"<p><code>DynMatrix&lt;T&gt;</code> is a heap-allocated matrix with runtime dimensions \u2014 the dynamic counterpart to <code>Matrix&lt;T, M, N&gt;</code>. Requires the <code>alloc</code> feature (enabled by default via <code>std</code>).</p>"},{"location":"dynmatrix/#storage-layout","title":"Storage Layout","text":"<p><code>DynMatrix&lt;T&gt;</code> stores elements in a <code>Vec&lt;T&gt;</code> in column-major order: element <code>(row, col)</code> is at index <code>col * nrows + row</code>. This matches the fixed <code>Matrix</code> layout and enables the same SIMD inner-loop optimizations.</p> <ul> <li><code>from_rows()</code> accepts row-major input (transposes internally)</li> <li><code>from_slice()</code> accepts column-major input directly</li> </ul>"},{"location":"dynmatrix/#constructors","title":"Constructors","text":"<pre><code>use numeris::{DynMatrix, DynVector};\n\n// From row-major data\nlet a = DynMatrix::from_rows(3, 3, &amp;[\n    2.0_f64, 1.0, -1.0,\n    -3.0,   -1.0,  2.0,\n    -2.0,    1.0,  2.0,\n]);\n\n// From column-major data\nlet b = DynMatrix::from_slice(2, 3, &amp;[\n    1.0_f64, 4.0,   // col 0\n    2.0,     5.0,   // col 1\n    3.0,     6.0,   // col 2\n]);\n\n// All zeros / all ones\nlet z = DynMatrix::&lt;f64&gt;::zeros(4, 4);\nlet o = DynMatrix::&lt;f64&gt;::ones(2, 5);\n\n// Identity\nlet id = DynMatrix::&lt;f64&gt;::eye(3);\n\n// From function f(row, col)\nlet m = DynMatrix::&lt;f64&gt;::from_fn(3, 3, |r, c| (r * 3 + c) as f64);\n\n// Dynamic vector (enforces 1-column constraint)\nlet v = DynVector::from_slice(&amp;[1.0_f64, 2.0, 3.0]);\n</code></pre>"},{"location":"dynmatrix/#indexing-and-access","title":"Indexing and Access","text":"<pre><code>let a = DynMatrix::from_rows(2, 3, &amp;[1.0_f64, 2.0, 3.0, 4.0, 5.0, 6.0]);\n\n// Element access\nlet v = a[(0, 2)];          // row 0, col 2 \u2192 3.0\n\n// Dimensions\nlet r = a.nrows();          // 2\nlet c = a.ncols();          // 3\n\n// Mutable element\nlet mut b = a.clone();\nb[(1, 1)] = 99.0;\n\n// Dynamic vector single-index\nlet v = DynVector::from_slice(&amp;[10.0_f64, 20.0, 30.0]);\nlet elem = v[1];            // 20.0\n</code></pre>"},{"location":"dynmatrix/#arithmetic","title":"Arithmetic","text":"<pre><code>let a = DynMatrix::from_rows(2, 2, &amp;[1.0_f64, 3.0, 2.0, 4.0]);\nlet b = DynMatrix::from_rows(2, 2, &amp;[5.0_f64, 7.0, 6.0, 8.0]);\n\nlet c = &amp;a + &amp;b;            // element-wise add\nlet d = &amp;a - &amp;b;            // element-wise subtract\nlet e = &amp;a * &amp;b;            // matrix multiply\nlet f = -&amp;a;                // negation\n\nlet g = &amp;a * 2.0_f64;      // scalar multiply\nlet h = &amp;a / 2.0_f64;      // scalar divide\n\nlet p = a.element_mul(&amp;b);  // element-wise multiply\nlet q = a.element_div(&amp;b);  // element-wise divide\n\n// Transpose\nlet at = a.transpose();\n</code></pre>"},{"location":"dynmatrix/#mixed-operations-with-fixed-matrix","title":"Mixed Operations with Fixed Matrix","text":"<p><code>Matrix&lt;T, M, N&gt;</code> and <code>DynMatrix&lt;T&gt;</code> interoperate via <code>mixed_ops</code>:</p> <pre><code>use numeris::{Matrix, DynMatrix};\n\nlet fixed = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\nlet dyn_  = DynMatrix::from_rows(2, 2, &amp;[5.0_f64, 7.0, 6.0, 8.0]);\n\n// All combinations produce DynMatrix\nlet r1: DynMatrix&lt;f64&gt; = &amp;fixed * &amp;dyn_;   // Matrix * DynMatrix\nlet r2: DynMatrix&lt;f64&gt; = &amp;dyn_  * &amp;fixed;  // DynMatrix * Matrix\nlet r3: DynMatrix&lt;f64&gt; = &amp;fixed + &amp;dyn_;   // Matrix + DynMatrix\nlet r4: DynMatrix&lt;f64&gt; = &amp;dyn_  + &amp;fixed;  // DynMatrix + Matrix\n</code></pre>"},{"location":"dynmatrix/#conversions","title":"Conversions","text":"<pre><code>use numeris::{Matrix, DynMatrix};\n\n// Fixed \u2192 Dynamic (infallible)\nlet fixed = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\nlet dyn_: DynMatrix&lt;f64&gt; = fixed.into();\n\n// Dynamic \u2192 Fixed (fallible \u2014 panics if dimensions don't match)\nlet back: Matrix&lt;f64, 2, 2&gt; = (&amp;dyn_).try_into().unwrap();\n\n// DynVector \u2194 DynMatrix\nuse numeris::DynVector;\nlet v = DynVector::from_slice(&amp;[1.0_f64, 2.0, 3.0]);\nlet m: DynMatrix&lt;f64&gt; = v.into();  // 3\u00d71 matrix\n</code></pre>"},{"location":"dynmatrix/#norms-and-utilities","title":"Norms and Utilities","text":"<pre><code>let a = DynMatrix::from_rows(2, 2, &amp;[3.0_f64, 4.0, 0.0, 0.0]);\n\nlet frob = a.frobenius_norm();\nlet inf  = a.norm_inf();\nlet one  = a.norm_one();\nlet s    = a.sum();\nlet abs_ = a.abs();\nlet m    = a.map(|x| x * x);\n\n// Swap rows / columns\nlet mut b = a.clone();\nb.swap_rows(0, 1);\nb.swap_cols(0, 1);\n\n// Dynamic vector norms\nlet v = DynVector::from_slice(&amp;[3.0_f64, 4.0]);\nlet n  = v.norm();     // L2 = 5.0\nlet n1 = v.norm_l1();  // L1 = 7.0\nlet d  = v.dot(&amp;v);    // dot product = 25.0\n</code></pre>"},{"location":"dynmatrix/#block-operations","title":"Block Operations","text":"<pre><code>let big = DynMatrix::from_rows(4, 4, &amp;[\n     1.0_f64,  2.0,  3.0,  4.0,\n     5.0,  6.0,  7.0,  8.0,\n     9.0, 10.0, 11.0, 12.0,\n    13.0, 14.0, 15.0, 16.0,\n]);\n\n// Extract sub-block (row_start, col_start, nrows, ncols)\nlet sub = big.block(1, 1, 2, 2);  // [[6,7],[10,11]]\n\n// Insert sub-block\nlet mut m = DynMatrix::&lt;f64&gt;::zeros(4, 4);\nlet patch = DynMatrix::from_rows(2, 2, &amp;[9.0_f64, 8.0, 7.0, 6.0]);\nm.set_block(1, 1, &amp;patch);\n</code></pre>"},{"location":"dynmatrix/#square-operations","title":"Square Operations","text":"<pre><code>let a = DynMatrix::from_rows(3, 3, &amp;[\n    4.0_f64, 2.0, 0.0,\n    2.0,     3.0, 1.0,\n    0.0,     1.0, 2.0,\n]);\n\nlet tr  = a.trace();\nlet det = a.det();\nlet dg  = a.diag();         // DynVector\nlet sym = a.is_symmetric();\n</code></pre>"},{"location":"dynmatrix/#linear-algebra","title":"Linear Algebra","text":"<p>All decompositions work the same as on <code>Matrix</code> \u2014 the same free functions operate on both via <code>MatrixRef</code>/<code>MatrixMut</code> traits:</p> <pre><code>let a = DynMatrix::from_rows(3, 3, &amp;[\n    4.0_f64, 2.0, 0.0,\n    2.0,     3.0, 1.0,\n    0.0,     1.0, 2.0,\n]);\nlet b = DynVector::from_slice(&amp;[1.0_f64, 2.0, 3.0]);\n\nlet lu   = a.lu().unwrap();\nlet chol = a.cholesky().unwrap();\nlet qr   = a.qr().unwrap();\nlet svd  = a.svd().unwrap();\nlet eig  = a.eig_symmetric().unwrap();\nlet sch  = a.schur().unwrap();\n\nlet x   = a.solve(&amp;b).unwrap();\nlet inv = a.inverse().unwrap();\nlet det = a.det();\n</code></pre> <p>See Linear Algebra for full decomposition details.</p>"},{"location":"dynmatrix/#type-aliases","title":"Type Aliases","text":"Alias Type <code>DynMatrixf64</code> <code>DynMatrix&lt;f64&gt;</code> <code>DynMatrixf32</code> <code>DynMatrix&lt;f32&gt;</code> <code>DynVectorf64</code> <code>DynVector&lt;f64&gt;</code> <code>DynVectorf32</code> <code>DynVector&lt;f32&gt;</code> <code>DynMatrixi32</code> <code>DynMatrix&lt;i32&gt;</code> <code>DynMatrixi64</code> <code>DynMatrix&lt;i64&gt;</code> <code>DynMatrixu32</code> <code>DynMatrix&lt;u32&gt;</code> <code>DynMatrixu64</code> <code>DynMatrix&lt;u64&gt;</code> <code>DynMatrixz64</code> <code>DynMatrix&lt;Complex&lt;f64&gt;&gt;</code> (requires <code>complex</code>) <code>DynMatrixz32</code> <code>DynMatrix&lt;Complex&lt;f32&gt;&gt;</code> (requires <code>complex</code>)"},{"location":"estimate/","title":"State Estimation","text":"<p>Six estimators for nonlinear state estimation and offline batch processing. All use const-generic state (<code>N</code>) and measurement (<code>M</code>) dimensions with closure-based dynamics and measurement models.</p> <p>Requires the <code>estimate</code> Cargo feature (implies <code>alloc</code>):</p> <pre><code>numeris = { version = \"0.2\", features = [\"estimate\"] }\n</code></pre>"},{"location":"estimate/#filter-comparison","title":"Filter Comparison","text":"Filter Struct Jacobians No-std When to use Extended Kalman <code>Ekf&lt;T, N, M&gt;</code> User-supplied or FD Yes Linear-ish dynamics, fast, no-alloc Unscented Kalman <code>Ukf&lt;T, N, M&gt;</code> Not needed No (<code>alloc</code>) Moderately nonlinear dynamics Square-Root UKF <code>SrUkf&lt;T, N, M&gt;</code> Not needed No (<code>alloc</code>) UKF + guaranteed PD covariance Cubature Kalman <code>Ckf&lt;T, N, M&gt;</code> Not needed No (<code>alloc</code>) No tuning parameters, 2N points RTS Smoother <code>rts_smooth</code> From EKF forward pass No (<code>alloc</code>) Offline batch smoothing Batch Least-Squares <code>BatchLsq&lt;T, N&gt;</code> Linear H matrix Yes Linear observations, offline <p>All support <code>f32</code> and <code>f64</code>. Process noise <code>Q</code> is optional (<code>Some(&amp;q)</code> or <code>None</code>).</p>"},{"location":"estimate/#extended-kalman-filter-ekf","title":"Extended Kalman Filter (EKF)","text":"<p>The EKF linearizes nonlinear dynamics at the current estimate using Jacobians. Fully no-std and no-alloc.</p> <pre><code>use numeris::estimate::Ekf;\nuse numeris::{ColumnVector, Matrix};\n\n// 2-state constant-velocity model, 1-dimensional position measurement\nlet x0 = ColumnVector::from_column([0.0_f64, 1.0]);  // [pos, vel]\nlet p0 = Matrix::new([[1.0, 0.0], [0.0, 1.0]]);\nlet mut ekf = Ekf::&lt;f64, 2, 1&gt;::new(x0, p0);\n\nlet dt = 0.1_f64;\nlet q = Some(Matrix::new([[0.01_f64, 0.0], [0.0, 0.01]]));  // process noise\nlet r = Matrix::new([[0.5_f64]]);                              // measurement noise\n\n// Prediction step: provide dynamics f(x) and Jacobian F = \u2202f/\u2202x\nekf.predict(\n    |x| ColumnVector::from_column([\n        x[(0, 0)] + dt * x[(1, 0)],\n        x[(1, 0)],\n    ]),\n    |_x| Matrix::new([[1.0_f64, dt], [0.0, 1.0]]),\n    q.as_ref(),\n);\n\n// Update step: provide measurement h(x) and Jacobian H = \u2202h/\u2202x\nekf.update(\n    &amp;ColumnVector::from_column([0.12_f64]),  // measurement z\n    |x| ColumnVector::from_column([x[(0, 0)]]),\n    |_x| Matrix::new([[1.0_f64, 0.0]]),\n    &amp;r,\n).unwrap();\n\n// Access state and covariance\nlet x = ekf.state();       // ColumnVector&lt;f64, 2&gt;\nlet p = ekf.covariance();  // Matrix&lt;f64, 2, 2&gt;\n</code></pre>"},{"location":"estimate/#finite-difference-jacobians","title":"Finite-Difference Jacobians","text":"<p>When analytic Jacobians are unavailable, use <code>predict_fd</code> / <code>update_fd</code>:</p> <pre><code>ekf.predict_fd(\n    |x| ColumnVector::from_column([x[(0,0)] + dt*x[(1,0)], x[(1,0)]]),\n    q.as_ref(),\n);\nekf.update_fd(\n    &amp;ColumnVector::from_column([0.12]),\n    |x| ColumnVector::from_column([x[(0,0)]]),\n    &amp;r,\n).unwrap();\n</code></pre> <p>Finite differences use forward differences with step size <code>\u221a\u03b5</code> (\u2248 1.5\u00d710\u207b\u2078 for <code>f64</code>).</p>"},{"location":"estimate/#unscented-kalman-filter-ukf","title":"Unscented Kalman Filter (UKF)","text":"<p>Propagates 2N+1 sigma points through the nonlinear dynamics \u2014 no Jacobians needed. Uses Merwe-scaled sigma points with tunable parameters.</p> <pre><code>use numeris::estimate::Ukf;\nuse numeris::{ColumnVector, Matrix};\n\nlet mut ukf = Ukf::&lt;f64, 2, 1&gt;::new(x0, p0);\n\n// Optional tuning: alpha (spread), beta (prior on distribution), kappa\n// Default: alpha=0.001, beta=2.0, kappa=0.0 (good for most cases)\nlet mut ukf_tuned = Ukf::&lt;f64, 2, 1&gt;::with_params(x0, p0, 0.001, 2.0, 0.0);\n\nukf.predict(\n    |x| ColumnVector::from_column([x[(0,0)] + dt*x[(1,0)], x[(1,0)]]),\n    q.as_ref(),\n).unwrap();\n\nukf.update(\n    &amp;ColumnVector::from_column([0.12]),\n    |x| ColumnVector::from_column([x[(0,0)]]),\n    &amp;r,\n).unwrap();\n</code></pre>"},{"location":"estimate/#square-root-ukf-sr-ukf","title":"Square-Root UKF (SR-UKF)","text":"<p>Propagates the Cholesky factor <code>S</code> (where <code>P = SS\u1d40</code>) rather than <code>P</code> directly. Guarantees positive-definiteness of the covariance even in the presence of numerical round-off.</p> <pre><code>use numeris::estimate::SrUkf;\nuse numeris::{ColumnVector, Matrix};\n\n// Construct from covariance P (Cholesky computed internally)\nlet mut srukf = SrUkf::&lt;f64, 2, 1&gt;::from_covariance(x0, p0);\n\n// Or from the Cholesky factor S directly\nlet s0 = p0.cholesky().unwrap().l();  // lower-triangular factor\nlet mut srukf2 = SrUkf::&lt;f64, 2, 1&gt;::new(x0, s0);\n\nsrukf.predict(\n    |x| ColumnVector::from_column([x[(0,0)] + dt*x[(1,0)], x[(1,0)]]),\n    q.as_ref(),\n).unwrap();\n\nsrukf.update(\n    &amp;ColumnVector::from_column([0.12]),\n    |x| ColumnVector::from_column([x[(0,0)]]),\n    &amp;r,\n).unwrap();\n\nlet p = srukf.covariance();  // reconstructed P = SS\u1d40\n</code></pre>"},{"location":"estimate/#cubature-kalman-filter-ckf","title":"Cubature Kalman Filter (CKF)","text":"<p>Third-degree spherical-radial cubature rule: 2N equally-weighted cubature points <code>x \u00b1 \u221aN \u00b7 L \u00b7 e\u1d62</code> where <code>L = chol(P)</code>. No tuning parameters.</p> <pre><code>use numeris::estimate::Ckf;\nuse numeris::{ColumnVector, Matrix};\n\nlet mut ckf = Ckf::&lt;f64, 2, 1&gt;::new(x0, p0);\n\nckf.predict(\n    |x| ColumnVector::from_column([x[(0,0)] + dt*x[(1,0)], x[(1,0)]]),\n    q.as_ref(),\n).unwrap();\n\nckf.update(\n    &amp;ColumnVector::from_column([0.12]),\n    |x| ColumnVector::from_column([x[(0,0)]]),\n    &amp;r,\n).unwrap();\n</code></pre>"},{"location":"estimate/#rts-smoother","title":"RTS Smoother","text":"<p>Rauch\u2013Tung\u2013Striebel fixed-interval smoother: runs an EKF forward pass, stores <code>EkfStep</code> records, then runs a backward pass to produce smoothed estimates. Smoothed covariance is always \u2264 filtered covariance.</p> <pre><code>use numeris::estimate::{Ekf, EkfStep, rts_smooth};\nuse numeris::{ColumnVector, Matrix};\n\n// Forward EKF pass \u2014 store each step\nlet mut ekf = Ekf::&lt;f64, 2, 1&gt;::new(x0, p0);\nlet mut steps: Vec&lt;EkfStep&lt;f64, 2&gt;&gt; = Vec::new();\n\nfor z in &amp;measurements {\n    let step = ekf.predict_store(\n        |x| ColumnVector::from_column([x[(0,0)] + dt*x[(1,0)], x[(1,0)]]),\n        |_x| Matrix::new([[1.0_f64, dt], [0.0, 1.0]]),\n        q.as_ref(),\n    );\n    steps.push(step);\n\n    ekf.update(\n        &amp;ColumnVector::from_column([*z]),\n        |x| ColumnVector::from_column([x[(0,0)]]),\n        |_x| Matrix::new([[1.0_f64, 0.0]]),\n        &amp;r,\n    ).unwrap();\n}\n\n// Backward RTS smoother pass\nlet smoothed = rts_smooth(&amp;steps).unwrap();\n// smoothed[k].x \u2014 smoothed state at step k\n// smoothed[k].p \u2014 smoothed covariance at step k\n</code></pre>"},{"location":"estimate/#batch-least-squares","title":"Batch Least-Squares","text":"<p>Information-form batch least-squares: accumulate <code>\u039b = \u03a3 H\u1d40R\u207b\u00b9H</code>, <code>\u03b7 = \u03a3 H\u1d40R\u207b\u00b9z</code>, then solve <code>\u039bx = \u03b7</code>. Fully no-std, no-alloc.</p> <p>Supports mixed measurement dimensions via method-level const generic <code>M</code>.</p> <pre><code>use numeris::estimate::BatchLsq;\nuse numeris::{ColumnVector, Matrix};\n\nlet mut lsq = BatchLsq::&lt;f64, 2&gt;::new();  // 2-state\n\n// Add observations with (possibly different) H dimensions\nlet h1 = Matrix::new([[1.0_f64, 0.0]]);    // 1\u00d72: observe position\nlet h2 = Matrix::new([[0.0_f64, 1.0]]);    // 1\u00d72: observe velocity\nlet r  = Matrix::new([[0.1_f64]]);\n\nlsq.add_observation(&amp;ColumnVector::from_column([1.05_f64]), &amp;h1, &amp;r).unwrap();\nlsq.add_observation(&amp;ColumnVector::from_column([0.95_f64]), &amp;h1, &amp;r).unwrap();\nlsq.add_observation(&amp;ColumnVector::from_column([1.0_f64]),  &amp;h2, &amp;r).unwrap();\n\n// Solve: returns (state estimate, covariance)\nlet (x_est, p_est) = lsq.solve().unwrap();\n\n// With a prior (Tikhonov regularization):\nlet x_prior = ColumnVector::from_column([0.0_f64, 0.0]);\nlet p_prior  = Matrix::new([[1.0_f64, 0.0], [0.0, 1.0]]);\nlet mut lsq_prior = BatchLsq::&lt;f64, 2&gt;::with_prior(&amp;x_prior, &amp;p_prior).unwrap();\n</code></pre>"},{"location":"estimate/#choosing-a-filter","title":"Choosing a Filter","text":"<pre><code>Are your dynamics highly nonlinear?\n  \u2192 No:  EKF (fast, no-alloc, good enough for mildly nonlinear systems)\n  \u2192 Yes: UKF or CKF\n\nDo you need guaranteed positive-definite covariance in long runs?\n  \u2192 SrUkf (propagates Cholesky factor)\n\nDo you want zero tuning parameters?\n  \u2192 CKF (2N points, equal weights)\n\nAre you doing offline post-processing with all data available?\n  \u2192 RTS smoother (better estimates than forward-only filtering)\n\nAre your measurements linear in the state?\n  \u2192 BatchLsq (simple, no-alloc, optimal for linear-Gaussian problems)\n</code></pre>"},{"location":"estimate/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::estimate::EstimateError;\n\nmatch ekf.update(&amp;z, h, hj, &amp;r) {\n    Ok(())                                      =&gt; { /* success */ }\n    Err(EstimateError::CovarianceNotPD)         =&gt; { /* P lost positive-definiteness */ }\n    Err(EstimateError::SingularInnovation)      =&gt; { /* S = HPH\u1d40 + R is singular */ }\n    Err(EstimateError::CholdowndateFailed)      =&gt; { /* SR-UKF Cholesky downdate failed */ }\n}\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Add numeris to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\nnumeris = \"0.2\"\n</code></pre> <p>The default features include <code>std</code> and <code>ode</code>. To enable additional modules, list them explicitly:</p> <pre><code>[dependencies]\nnumeris = { version = \"0.2\", features = [\"optim\", \"control\", \"estimate\", \"interp\", \"special\", \"stats\", \"complex\"] }\n\n# Or enable everything at once:\nnumeris = { version = \"0.2\", features = [\"all\"] }\n</code></pre>"},{"location":"getting-started/#cargo-features","title":"Cargo Features","text":"Feature Default Description <code>std</code> yes Implies <code>alloc</code>. Uses hardware FPU via system libm. Full float speed. <code>alloc</code> via <code>std</code> Enables <code>DynMatrix</code> / <code>DynVector</code> (heap-allocated, runtime-sized). <code>ode</code> yes ODE integration \u2014 RK4, 7 adaptive solvers, RODAS4 stiff solver. <code>optim</code> no Optimization \u2014 root finding, BFGS, Gauss-Newton, Levenberg-Marquardt. <code>control</code> no Digital IIR filters (Butterworth, Chebyshev Type I) and PID controller. <code>estimate</code> no State estimation \u2014 EKF, UKF, SR-UKF, CKF, RTS smoother, batch LSQ. Implies <code>alloc</code>. <code>interp</code> no Interpolation \u2014 linear, Hermite, Lagrange, cubic spline, bilinear. <code>special</code> no Special functions \u2014 gamma, lgamma, digamma, beta, betainc, erf. <code>stats</code> no Statistical distributions (10 families). Implies <code>special</code>. <code>complex</code> no <code>Complex&lt;f32&gt;</code> / <code>Complex&lt;f64&gt;</code> support for all decompositions. <code>libm</code> baseline Pure-Rust software float math. Always on as fallback. <code>all</code> no All of the above."},{"location":"getting-started/#build-variants","title":"Build Variants","text":"<pre><code># Default (std + ode)\ncargo build\n\n# All features \u2014 the kitchen sink\ncargo build --features all\n\n# No-std embedded target\ncargo build --no-default-features --features libm\n\n# No-std with heap (DynMatrix available)\ncargo build --no-default-features --features \"libm,alloc\"\n\n# Optimization + complex numbers\ncargo build --features \"optim,complex\"\n\n# State estimation (also enables DynMatrix)\ncargo build --features estimate\n</code></pre>"},{"location":"getting-started/#simd-acceleration","title":"SIMD Acceleration","text":"<p>SIMD is always-on for <code>f32</code> and <code>f64</code> \u2014 no feature flag required.</p> <ul> <li>aarch64: NEON intrinsics, always available</li> <li>x86_64: SSE2 always available; AVX and AVX-512 via compiler flags</li> </ul> <p>To enable AVX/AVX-512 on x86_64:</p> <pre><code># Enable all native CPU features (recommended for desktop/server)\nRUSTFLAGS=\"-C target-cpu=native\" cargo build --release\n\n# Or explicitly\nRUSTFLAGS=\"-C target-feature=+avx2,+avx512f\" cargo build --release\n</code></pre>"},{"location":"getting-started/#first-examples","title":"First Examples","text":""},{"location":"getting-started/#matrix-arithmetic","title":"Matrix arithmetic","text":"<pre><code>use numeris::{Matrix, Vector, Matrix3};\n\n// Matrix creation \u2014 new() takes row-major input, stores column-major\nlet a = Matrix::new([\n    [1.0_f64, 2.0, 3.0],\n    [4.0,     5.0, 6.0],\n    [7.0,     8.0, 9.0],\n]);\n\n// Size aliases\nlet id: Matrix3&lt;f64&gt; = Matrix3::eye();\nlet z: Matrix3&lt;f64&gt;  = Matrix3::zeros();\n\n// Arithmetic\nlet b = a * id;          // matrix multiply\nlet c = a + &amp;b;          // element-wise add\nlet d = a * 2.0;         // scalar multiply\n\n// Vectors\nlet v = Vector::from_array([1.0_f64, 0.0, 0.0]);\nlet w = a.vecmul(&amp;v);    // A * v (matrix-vector)\nlet dot = v.dot(&amp;w);\n\n// Indexing\nlet elem = a[(1, 2)];    // row 1, col 2 = 6.0\n</code></pre>"},{"location":"getting-started/#linear-system-solve","title":"Linear system solve","text":"<pre><code>use numeris::{Matrix, Vector};\n\nlet a = Matrix::new([\n    [2.0_f64, 1.0, -1.0],\n    [-3.0,   -1.0,  2.0],\n    [-2.0,    1.0,  2.0],\n]);\nlet b = Vector::from_array([8.0, -11.0, -3.0]);\n\n// High-level convenience\nlet x = a.solve(&amp;b).unwrap();\nassert!((x[0] - 2.0).abs() &lt; 1e-12);\nassert!((x[1] - 3.0).abs() &lt; 1e-12);\nassert!((x[2] + 1.0).abs() &lt; 1e-12);\n\n// Or access the decomposition directly\nlet lu = a.lu().unwrap();\nlet x2 = lu.solve(&amp;b);\nlet inv = lu.inverse();\nlet det = lu.det();\n</code></pre>"},{"location":"getting-started/#ode-integration","title":"ODE integration","text":"<pre><code>use numeris::ode::{RKAdaptive, RKTS54, AdaptiveSettings};\nuse numeris::Vector;\n\n// Simple harmonic oscillator: [x, v]' = [v, -x]\nlet y0 = Vector::from_array([1.0_f64, 0.0]);  // x=1, v=0\nlet tau = 2.0 * std::f64::consts::PI;\n\nlet sol = RKTS54::integrate(\n    0.0, tau, &amp;y0,\n    |_t, y| Vector::from_array([y[1], -y[0]]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\n\n// After one full period, x \u2248 1, v \u2248 0\nassert!((sol.y[0] - 1.0).abs() &lt; 1e-6);\nassert!(sol.y[1].abs() &lt; 1e-6);\n</code></pre>"},{"location":"getting-started/#dynamic-matrices","title":"Dynamic matrices","text":"<pre><code>use numeris::{DynMatrix, DynVector};\n\nlet a = DynMatrix::from_rows(3, 3, &amp;[\n    2.0_f64, 1.0, -1.0,\n    -3.0,   -1.0,  2.0,\n    -2.0,    1.0,  2.0,\n]);\nlet b = DynVector::from_slice(&amp;[8.0, -11.0, -3.0]);\nlet x = a.solve(&amp;b).unwrap();\n</code></pre> <p>See the Matrix and DynMatrix pages for the full API.</p>"},{"location":"interp/","title":"Interpolation","text":"<p>Four interpolation methods plus bilinear 2D interpolation. Each method has a fixed-size variant (stack-allocated, const-generic number of knots) and a dynamic variant (heap-allocated, requires <code>alloc</code>).</p> <p>Requires the <code>interp</code> Cargo feature:</p> <pre><code>numeris = { version = \"0.2\", features = [\"interp\"] }\n</code></pre>"},{"location":"interp/#method-summary","title":"Method Summary","text":"Method Struct Continuity Best for Linear <code>LinearInterp</code> C\u2070 Fast, piecewise linear data Hermite <code>HermiteInterp</code> C\u00b9 Data with known derivatives Lagrange <code>LagrangeInterp</code> C\u207f\u207b\u00b9 Global polynomial through all points Cubic Spline <code>CubicSpline</code> C\u00b2 Smooth curves, natural boundary conditions Bilinear <code>BilinearInterp</code> C\u2070 2D rectangular grid data <p>Dynamic variants: <code>DynLinearInterp</code>, <code>DynHermiteInterp</code>, <code>DynLagrangeInterp</code>, <code>DynCubicSpline</code>, <code>DynBilinearInterp</code>.</p>"},{"location":"interp/#linear-interpolation","title":"Linear Interpolation","text":"<p>Piecewise linear interpolation between knots. O(log N) per evaluation (binary search).</p> <p>Out-of-bounds queries extrapolate using the nearest segment.</p> <pre><code>use numeris::interp::LinearInterp;\n\n// Fixed-size: 4 knots\nlet xs = [0.0_f64, 1.0, 2.0, 3.0];\nlet ys = [0.0_f64, 1.0, 0.0, 1.0];\nlet interp = LinearInterp::new(&amp;xs, &amp;ys).unwrap();\n\nlet y = interp.eval(1.5);  // 0.5\n</code></pre> <p>Dynamic variant:</p> <pre><code>use numeris::interp::DynLinearInterp;\n\nlet interp = DynLinearInterp::new(&amp;xs, &amp;ys).unwrap();\nlet y = interp.eval(1.5);\n</code></pre>"},{"location":"interp/#hermite-interpolation","title":"Hermite Interpolation","text":"<p>Cubic Hermite interpolation with user-supplied derivatives at each knot. Guarantees C\u00b9 continuity.</p> <pre><code>use numeris::interp::HermiteInterp;\n\nlet xs = [0.0_f64, 1.0, 2.0, 3.0];\nlet ys = [0.0_f64, 1.0, 0.0, 1.0];\n// Derivatives at each knot (e.g., from finite differences or analytic formula)\nlet ds = [1.0_f64, 0.0, -1.0, 0.0];\n\nlet interp = HermiteInterp::new(&amp;xs, &amp;ys, &amp;ds).unwrap();\n\nlet y  = interp.eval(1.5);\nlet dy = interp.eval_deriv(1.5);  // first derivative\n</code></pre> <p>When to use Hermite</p> <p>Use Hermite when you have physical knowledge of derivatives \u2014 e.g., velocity data from an accelerometer alongside position data. It avoids Runge's phenomenon better than high-degree Lagrange polynomials.</p>"},{"location":"interp/#barycentric-lagrange-interpolation","title":"Barycentric Lagrange Interpolation","text":"<p>Global polynomial interpolation through all N knots. O(N\u00b2) setup (barycentric weights), O(N) per evaluation. C^(N-1) continuity.</p> <pre><code>use numeris::interp::LagrangeInterp;\n\nlet xs = [0.0_f64, 0.5, 1.0, 1.5, 2.0];\nlet ys = [0.0_f64, 0.479, 0.841, 0.997, 0.909];  // \u2248 sin(x)\n\nlet interp = LagrangeInterp::new(&amp;xs, &amp;ys).unwrap();\n\nlet y  = interp.eval(0.75);\nlet dy = interp.eval_deriv(0.75);  // first derivative\n</code></pre> <p>Runge's phenomenon</p> <p>High-degree Lagrange polynomials on uniformly-spaced nodes can oscillate wildly near the boundaries. Prefer cubic splines for smooth data with many knots, or use Chebyshev nodes for Lagrange when possible.</p>"},{"location":"interp/#natural-cubic-spline","title":"Natural Cubic Spline","text":"<p>Piecewise cubic with C\u00b2 continuity (continuous second derivative). Natural boundary conditions (<code>S''(x\u2080) = S''(x\u2099) = 0</code>). Solved via Thomas algorithm (tridiagonal, O(N)).</p> <pre><code>use numeris::interp::CubicSpline;\n\nlet xs = [0.0_f64, 1.0, 2.0, 3.0, 4.0];\nlet ys = [0.0_f64, 1.0, 0.0, 1.0, 0.0];\n\nlet spline = CubicSpline::new(&amp;xs, &amp;ys).unwrap();\n\nlet y  = spline.eval(1.5);       // smooth interpolation\nlet dy = spline.eval_deriv(1.5); // first derivative\n</code></pre> <p>Dynamic variant:</p> <pre><code>use numeris::interp::DynCubicSpline;\n\nlet spline = DynCubicSpline::new(&amp;xs, &amp;ys).unwrap();\nlet y = spline.eval(2.7);\n</code></pre>"},{"location":"interp/#bilinear-interpolation-2d","title":"Bilinear Interpolation (2D)","text":"<p>For data on a rectangular grid. Interpolates within each cell using the four corner values.</p> <pre><code>use numeris::interp::BilinearInterp;\n\n// Grid: 3 x-nodes \u00d7 4 y-nodes\nlet xs = [0.0_f64, 1.0, 2.0];\nlet ys = [0.0_f64, 1.0, 2.0, 3.0];\n\n// Values at each (xi, yj) node \u2014 stored row-by-row (x varies fastest)\n// z[i*ny + j] = f(xs[i], ys[j])\nlet zs = [\n    0.0_f64, 1.0, 2.0, 3.0,   // xs[0]=0 row\n    1.0,     2.0, 3.0, 4.0,   // xs[1]=1 row\n    4.0,     5.0, 6.0, 7.0,   // xs[2]=2 row\n];\n\nlet interp = BilinearInterp::new(&amp;xs, &amp;ys, &amp;zs).unwrap();\n\nlet z = interp.eval(0.5, 1.5);  // interpolate at (x=0.5, y=1.5)\n</code></pre> <p>Dynamic variant:</p> <pre><code>use numeris::interp::DynBilinearInterp;\n\nlet interp = DynBilinearInterp::new(&amp;xs, &amp;ys, &amp;zs).unwrap();\nlet z = interp.eval(0.5, 1.5);\n</code></pre>"},{"location":"interp/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::interp::InterpError;\n\nmatch CubicSpline::new(&amp;xs, &amp;ys) {\n    Ok(spline) =&gt; { /* use spline */ }\n    Err(InterpError::NotSorted)         =&gt; { /* xs must be strictly increasing */ }\n    Err(InterpError::LengthMismatch)    =&gt; { /* xs.len() != ys.len() */ }\n    Err(InterpError::InsufficientPoints) =&gt; { /* need at least 2 knots */ }\n}\n</code></pre>"},{"location":"interp/#method-comparison","title":"Method Comparison","text":"<pre><code>use numeris::interp::{LinearInterp, HermiteInterp, LagrangeInterp, CubicSpline};\n\nlet xs = [0.0_f64, 1.0, 2.0, 3.0];\nlet ys = [0.0_f64, 0.841, 0.909, 0.141];  // \u2248 sin(x)\nlet ds = [1.0_f64, 0.540, -0.416, -0.990]; // \u2248 cos(x) (derivatives)\n\nlet linear  = LinearInterp::new(&amp;xs, &amp;ys).unwrap();\nlet hermite = HermiteInterp::new(&amp;xs, &amp;ys, &amp;ds).unwrap();\nlet lagrange = LagrangeInterp::new(&amp;xs, &amp;ys).unwrap();\nlet spline  = CubicSpline::new(&amp;xs, &amp;ys).unwrap();\n\nlet x_query = 1.5;\nprintln!(\"linear   = {:.6}\", linear.eval(x_query));   // piecewise linear\nprintln!(\"hermite  = {:.6}\", hermite.eval(x_query));  // smooth, uses derivatives\nprintln!(\"lagrange = {:.6}\", lagrange.eval(x_query)); // global polynomial\nprintln!(\"spline   = {:.6}\", spline.eval(x_query));   // C\u00b2 cubic spline\n// All should be close to sin(1.5) \u2248 0.997\n</code></pre>"},{"location":"linalg/","title":"Linear Algebra","text":"<p>numeris provides six matrix decompositions, each available as:</p> <ul> <li>Free functions operating in-place on <code>&amp;mut impl MatrixMut&lt;T&gt;</code> \u2014 work on both <code>Matrix</code> and <code>DynMatrix</code></li> <li>Wrapper structs with <code>solve()</code>, <code>inverse()</code>, <code>det()</code> convenience methods</li> <li>Matrix convenience methods \u2014 <code>a.lu()</code>, <code>a.cholesky()</code>, etc.</li> </ul>"},{"location":"linalg/#overview","title":"Overview","text":"Decomposition Struct Dynamic wrapper Use case LU <code>LuDecomposition</code> <code>DynLu</code> General square systems, det, inverse Cholesky <code>CholeskyDecomposition</code> <code>DynCholesky</code> SPD / Hermitian PD systems QR <code>QrDecomposition</code> <code>DynQr</code> Least-squares, orthogonal basis SVD <code>SvdDecomposition</code> <code>DynSvd</code> Rank, condition number, pseudoinverse Symmetric Eigen <code>SymmetricEigen</code> <code>DynSymmetricEigen</code> Real eigenvalues, eigenvectors Schur <code>SchurDecomposition</code> <code>DynSchur</code> General eigenvalues (complex pairs)"},{"location":"linalg/#lu-decomposition","title":"LU Decomposition","text":"<p>Partial pivoting LU: <code>P A = L U</code>.</p> <pre><code>use numeris::{Matrix, Vector};\n\nlet a = Matrix::new([\n    [2.0_f64, 1.0, -1.0],\n    [-3.0,   -1.0,  2.0],\n    [-2.0,    1.0,  2.0],\n]);\nlet b = Vector::from_array([8.0, -11.0, -3.0]);\n\nlet lu = a.lu().unwrap();\n\n// Solve Ax = b\nlet x   = lu.solve(&amp;b);       // x = [2, 3, -1]\n\n// Other operations\nlet inv = lu.inverse();        // A^{-1}\nlet det = lu.det();            // determinant\n\n// Direct solve on Matrix (uses LU internally)\nlet x2 = a.solve(&amp;b).unwrap();\n</code></pre> <p>Small matrices</p> <p>For N \u2264 4, <code>inverse()</code> and <code>det()</code> use direct closed-form formulas (adjugate method), bypassing LU entirely. This is 2\u20133x faster than the general path.</p>"},{"location":"linalg/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>For symmetric positive-definite (SPD) matrices: <code>A = L L\u1d34</code>.</p> <p>Works with both real and complex (Hermitian positive-definite) matrices.</p> <pre><code>use numeris::Matrix;\n\nlet a = Matrix::new([\n    [4.0_f64, 2.0],\n    [2.0,     3.0],\n]);\n\nlet chol = a.cholesky().unwrap();\n\n// Solve Ax = b (forward + back substitution, no LU needed)\nlet b = numeris::Vector::from_array([1.0_f64, 2.0]);\nlet x = chol.solve(&amp;b);\n\n// Other operations\nlet inv    = chol.inverse();\nlet det    = chol.det();\nlet ln_det = chol.ln_det();    // log-determinant (stable for large matrices)\n</code></pre>"},{"location":"linalg/#qr-decomposition","title":"QR Decomposition","text":"<p>Householder QR: <code>A = Q R</code>.</p> <pre><code>use numeris::{Matrix, Vector};\n\n// Square system \u2014 exact solve\nlet a = Matrix::new([[2.0_f64, 1.0], [1.0, 3.0]]);\nlet b = Vector::from_array([5.0_f64, 10.0]);\nlet qr = a.qr().unwrap();\nlet x  = qr.solve(&amp;b);\n\n// Overdetermined system \u2014 least-squares\nlet a_rect = Matrix::new([\n    [1.0_f64, 0.0],\n    [1.0,     1.0],\n    [1.0,     2.0],\n]);\nlet b_rect = Vector::from_array([1.0_f64, 2.0, 4.0]);\nlet x_ls = a_rect.qr().unwrap().solve(&amp;b_rect);\n\n// Determinant\nlet det = a.qr().unwrap().det();\n</code></pre>"},{"location":"linalg/#svd","title":"SVD","text":"<p>Golub-Kahan implicit-shift QR on a bidiagonal form.</p> <p><code>A = U \u03a3 V\u1d40</code>, where <code>A</code> is <code>M \u00d7 N</code> with <code>M \u2265 N</code>.</p> <pre><code>use numeris::Matrix;\n\nlet a = Matrix::new([\n    [1.0_f64, 2.0],\n    [3.0,     4.0],\n    [5.0,     6.0],\n]);\n\nlet svd = a.svd().unwrap();\n\n// Components\nlet sigma = svd.singular_values();   // [\u03c3\u2080, \u03c3\u2081] sorted descending\nlet u = svd.u();                      // 3\u00d73 orthogonal (or 3\u00d72 thin)\nlet vt = svd.vt();                    // 2\u00d72 orthogonal (V\u1d40)\n\n// Rank and condition number\nlet rank = svd.rank(1e-10);          // rank with tolerance\nlet cond = svd.condition_number();   // \u03c3_max / \u03c3_min\n\n// Shortcut: only singular values (faster, no U or V)\nlet sigma_only = a.singular_values_only().unwrap();\n</code></pre> <p>M &lt; N matrices</p> <p><code>DynSvd</code> handles <code>M &lt; N</code> by transposing internally. The fixed <code>SvdDecomposition</code> requires <code>M \u2265 N</code> at compile time.</p>"},{"location":"linalg/#symmetric-eigendecomposition","title":"Symmetric Eigendecomposition","text":"<p>Householder tridiagonalization + implicit QR with Wilkinson shift.</p> <p>For real symmetric (or Hermitian) matrices only. Produces real eigenvalues.</p> <pre><code>use numeris::Matrix;\n\nlet a = Matrix::new([\n    [4.0_f64, 1.0, 0.0],\n    [1.0,     3.0, 1.0],\n    [0.0,     1.0, 2.0],\n]);\n\nlet eig = a.eig_symmetric().unwrap();\n\nlet vals = eig.eigenvalues();    // sorted ascending\nlet vecs = eig.eigenvectors();   // columns = orthonormal eigenvectors\n// Reconstruction: A \u2248 vecs * diag(vals) * vecs^T\n\n// Shortcut: eigenvalues only (faster, no eigenvectors)\nlet vals_only = a.eigenvalues_symmetric().unwrap();\n</code></pre>"},{"location":"linalg/#schur-decomposition","title":"Schur Decomposition","text":"<p>Francis double-shift QR on the upper Hessenberg form.</p> <p>Produces quasi-upper-triangular <code>S</code> with 1\u00d71 (real eigenvalue) and 2\u00d72 (conjugate complex pair) diagonal blocks, and orthogonal <code>Q</code> such that <code>A = Q S Q\u1d40</code>.</p> <pre><code>use numeris::Matrix;\n\n// 90\u00b0 rotation \u2014 eigenvalues \u00b1i\nlet a = Matrix::new([[0.0_f64, -1.0], [1.0, 0.0]]);\n\nlet schur = a.schur().unwrap();\nlet s = schur.s();   // quasi-upper-triangular\nlet q = schur.q();   // orthogonal factor\n\n// Extract eigenvalues (real and imaginary parts)\nlet (re, im) = a.eigenvalues().unwrap();\n// re = [0, 0], im = [1, -1]  (conjugate pair \u00b1i)\n</code></pre>"},{"location":"linalg/#dynmatrix-variants","title":"DynMatrix Variants","text":"<p>All six decompositions have dynamic wrappers in <code>DynMatrix</code>:</p> <pre><code>use numeris::{DynMatrix, DynVector};\n\nlet a = DynMatrix::from_rows(3, 3, &amp;[\n    4.0_f64, 2.0, 0.0,\n    2.0,     3.0, 1.0,\n    0.0,     1.0, 2.0,\n]);\nlet b = DynVector::from_slice(&amp;[6.0_f64, 7.0, 3.0]);\n\nlet lu   = a.lu().unwrap();\nlet chol = a.cholesky().unwrap();\nlet qr   = a.qr().unwrap();\nlet svd  = a.svd().unwrap();\nlet eig  = a.eig_symmetric().unwrap();\nlet sch  = a.schur().unwrap();\n\nlet x   = a.solve(&amp;b).unwrap();\nlet inv = a.inverse().unwrap();\n</code></pre>"},{"location":"linalg/#complex-matrices","title":"Complex Matrices","text":"<p>Enable the <code>complex</code> feature to use decompositions with complex elements:</p> <pre><code>numeris = { version = \"0.2\", features = [\"complex\"] }\n</code></pre> <pre><code>use numeris::{Complex, Matrix, Vector};\ntype C = Complex&lt;f64&gt;;\n\nlet a = Matrix::new([\n    [C::new(2.0, 1.0), C::new(1.0, -1.0)],\n    [C::new(1.0, 0.0), C::new(3.0, 2.0)],\n]);\nlet b = Vector::from_array([C::new(5.0, 3.0), C::new(7.0, 4.0)]);\nlet x = a.solve(&amp;b).unwrap();\n\n// Hermitian positive-definite Cholesky (A = L L^H)\nlet hpd = Matrix::new([\n    [C::new(4.0, 0.0), C::new(2.0,  1.0)],\n    [C::new(2.0,-1.0), C::new(5.0,  0.0)],\n]);\nlet chol = hpd.cholesky().unwrap();\n</code></pre> <p>Complex overhead is zero for real code paths \u2014 <code>conj()</code> and <code>re()</code> on <code>f64</code> are identity functions, fully inlined and erased by the compiler.</p>"},{"location":"linalg/#error-handling","title":"Error Handling","text":"<p>All fallible operations return <code>Result&lt;_, LinalgError&gt;</code>:</p> <pre><code>use numeris::linalg::LinalgError;\n\nmatch a.cholesky() {\n    Ok(chol) =&gt; { /* use chol */ }\n    Err(LinalgError::NotPositiveDefinite) =&gt; { /* not SPD */ }\n    Err(LinalgError::SingularMatrix)      =&gt; { /* singular */ }\n    Err(LinalgError::ConvergenceFailure)  =&gt; { /* QR iteration didn't converge */ }\n    Err(e) =&gt; { /* other */ }\n}\n</code></pre>"},{"location":"matrix/","title":"Matrix","text":"<p><code>Matrix&lt;T, M, N&gt;</code> is the core fixed-size matrix type \u2014 stack-allocated, const-generic, column-major storage.</p>"},{"location":"matrix/#storage-layout","title":"Storage Layout","text":"<pre><code>Matrix&lt;T, M, N&gt;  \u2192  [[T; M]; N]   (N columns, each of length M)\n</code></pre> <p>Element <code>(row, col)</code> is at <code>data[col][row]</code>. This is column-major (Fortran / LAPACK order), which makes column-oriented inner loops \u2014 AXPY, Householder reflections, dot products \u2014 operate on contiguous memory.</p> <p><code>Matrix::new()</code> accepts row-major input (as you'd write on paper) and transposes internally:</p> <pre><code>use numeris::Matrix;\n\n// Written as rows, stored column-major\nlet a: Matrix&lt;f64, 2, 3&gt; = Matrix::new([\n    [1.0, 2.0, 3.0],   // row 0\n    [4.0, 5.0, 6.0],   // row 1\n]);\nassert_eq!(a[(0, 2)], 3.0);  // row 0, col 2\nassert_eq!(a[(1, 0)], 4.0);  // row 1, col 0\n</code></pre>"},{"location":"matrix/#constructors","title":"Constructors","text":"<pre><code>use numeris::{Matrix, Matrix3};\n\n// From row-major nested array\nlet a = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\n\n// All zeros / all ones / identity\nlet z = Matrix::&lt;f64, 3, 3&gt;::zeros();\nlet o = Matrix::&lt;f64, 2, 4&gt;::ones();\nlet id = Matrix3::&lt;f64&gt;::eye();\n\n// From function f(row, col)\nlet m = Matrix::&lt;f64, 3, 3&gt;::from_fn(|r, c| (r * 3 + c) as f64);\n\n// From column-major slice\nlet data = [1.0_f64, 4.0, 2.0, 5.0, 3.0, 6.0]; // col0=[1,4], col1=[2,5], col2=[3,6]\nlet m = Matrix::&lt;f64, 2, 3&gt;::from_col_slice(&amp;data);\n</code></pre>"},{"location":"matrix/#indexing","title":"Indexing","text":"<pre><code>let a = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\n\n// Element access\nlet v = a[(0, 1)];        // row 0, col 1 \u2192 2.0\n\n// Row / column access\nlet row = a.row(0);       // Matrix&lt;f64, 1, 2&gt;\nlet col = a.col(1);       // Matrix&lt;f64, 2, 1&gt;\n\n// Mutable element\nlet mut b = a;\nb[(1, 0)] = 99.0;\n</code></pre>"},{"location":"matrix/#arithmetic","title":"Arithmetic","text":"<pre><code>use numeris::{Matrix, Vector};\n\nlet a = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\nlet b = Matrix::new([[5.0_f64, 6.0], [7.0, 8.0]]);\n\n// Matrix operations\nlet c = a + b;              // element-wise add\nlet d = a - b;              // element-wise subtract\nlet e = a * b;              // matrix multiply (2\u00d72 * 2\u00d72)\nlet f = -a;                 // negation\n\n// Scalar operations\nlet g = a * 2.0;\nlet h = a / 2.0;\n\n// Matrix-vector multiply\nlet v = Vector::from_array([1.0_f64, 0.0]);\nlet w = a.vecmul(&amp;v);       // a * v \u2192 Vector&lt;f64, 2&gt;\n\n// Transpose\nlet at = a.transpose();     // Matrix&lt;f64, 2, 2&gt;\n\n// Element-wise multiply / divide\nlet p = a.element_mul(&amp;b);\nlet q = a.element_div(&amp;b);\n</code></pre>"},{"location":"matrix/#square-matrix-operations","title":"Square Matrix Operations","text":"<pre><code>use numeris::Matrix3;\n\nlet m = Matrix3::new([[4.0_f64, 2.0, 0.0], [2.0, 3.0, 1.0], [0.0, 1.0, 2.0]]);\n\nlet tr = m.trace();          // sum of diagonal = 9.0\nlet d  = m.det();            // determinant (via LU for N&gt;4, direct formula for N\u22644)\nlet dg = m.diag();           // diagonal as Vector&lt;f64, 3&gt;\nlet p2 = m.pow(2);           // matrix power (integer exponent)\nlet sym = m.is_symmetric();  // true/false\n\n// Construct diagonal matrix\nlet v = numeris::Vector::from_array([1.0_f64, 2.0, 3.0]);\nlet d = Matrix3::from_diag(&amp;v);\n</code></pre>"},{"location":"matrix/#norms","title":"Norms","text":"<pre><code>let a = Matrix::new([[3.0_f64, 0.0], [4.0, 0.0]]);\n\nlet frob = a.frobenius_norm();  // \u221a(9+16) = 5.0\nlet inf  = a.norm_inf();        // max row sum = 7.0\nlet one  = a.norm_one();        // max col sum = 7.0\n</code></pre>"},{"location":"matrix/#vectors","title":"Vectors","text":"<p><code>Vector&lt;T, N&gt;</code> is a row vector \u2014 a type alias for <code>Matrix&lt;T, 1, N&gt;</code>. <code>ColumnVector&lt;T, N&gt;</code> is a column vector \u2014 a type alias for <code>Matrix&lt;T, N, 1&gt;</code>.</p> <pre><code>use numeris::{Vector, ColumnVector, Vector3};\n\nlet v = Vector::from_array([3.0_f64, 4.0, 0.0]);\nlet u = Vector::from_array([1.0_f64, 0.0, 0.0]);\n\nlet d   = v.dot(&amp;u);           // 3.0\nlet n   = v.norm();            // 5.0\nlet n1  = v.norm_l1();         // 7.0\nlet hat = v.normalize();       // unit vector\n\nlet cross = v.cross(&amp;u);       // 3D cross product \u2192 Vector3&lt;f64&gt;\nlet outer = v.outer(&amp;u);       // outer product \u2192 Matrix&lt;f64, 3, 3&gt;\n\n// Column vector construction\nlet cv = ColumnVector::from_column([1.0_f64, 2.0, 3.0]);\n</code></pre>"},{"location":"matrix/#block-operations","title":"Block Operations","text":"<pre><code>use numeris::{Matrix, Matrix3};\n\nlet big = Matrix3::new([\n    [1.0_f64, 2.0, 3.0],\n    [4.0,     5.0, 6.0],\n    [7.0,     8.0, 9.0],\n]);\n\n// Extract sub-block starting at (row_start, col_start), size M\u00d7N\nlet sub: Matrix&lt;f64, 2, 2&gt; = big.block::&lt;2, 2&gt;(0, 0); // [[1,2],[4,5]]\n\n// Extract top-left / top-right / bottom-left / bottom-right corners\nlet tl: Matrix&lt;f64, 2, 2&gt; = big.top_left::&lt;2, 2&gt;();\nlet tr: Matrix&lt;f64, 2, 1&gt; = big.top_right::&lt;2, 1&gt;();\n\n// Head / tail for vectors\nuse numeris::Vector;\nlet v = Vector::from_array([1.0_f64, 2.0, 3.0, 4.0, 5.0]);\nlet h: numeris::Vector&lt;f64, 3&gt; = v.head::&lt;3&gt;();   // [1, 2, 3]\nlet t: numeris::Vector&lt;f64, 2&gt; = v.tail::&lt;2&gt;();   // [4, 5]\n\n// Insert sub-block\nlet mut m = Matrix3::&lt;f64&gt;::zeros();\nlet patch = Matrix::&lt;f64, 2, 2&gt;::new([[9.0, 8.0], [7.0, 6.0]]);\nm.set_block(1, 1, &amp;patch);\n</code></pre>"},{"location":"matrix/#size-aliases","title":"Size Aliases","text":"<p>Square matrices and row vectors up to 6\u00d76:</p> Square Rectangular (examples) Row vectors Column vectors <code>Matrix1&lt;T&gt;</code> <code>Matrix2x3&lt;T&gt;</code> <code>Vector1&lt;T&gt;</code> <code>ColumnVector1&lt;T&gt;</code> <code>Matrix2&lt;T&gt;</code> <code>Matrix3x4&lt;T&gt;</code> <code>Vector2&lt;T&gt;</code> <code>ColumnVector2&lt;T&gt;</code> <code>Matrix3&lt;T&gt;</code> <code>Matrix4x6&lt;T&gt;</code> <code>Vector3&lt;T&gt;</code> <code>ColumnVector3&lt;T&gt;</code> \u2026 up to <code>Matrix6&lt;T&gt;</code> All M\u00d7N for M,N \u2208 1..=6, M\u2260N \u2026 <code>Vector6&lt;T&gt;</code> \u2026 <code>ColumnVector6&lt;T&gt;</code> <pre><code>use numeris::{Matrix3, Matrix4x3, Vector3, ColumnVector3};\n\nlet rot: Matrix3&lt;f64&gt;     = Matrix3::eye();\nlet pts: Matrix4x3&lt;f64&gt;   = Matrix4x3::zeros();   // 4 rows, 3 cols\nlet v:   Vector3&lt;f64&gt;     = Vector3::from_array([1.0, 2.0, 3.0]);\nlet cv:  ColumnVector3&lt;f64&gt; = ColumnVector3::from_column([4.0, 5.0, 6.0]);\n</code></pre>"},{"location":"matrix/#utilities","title":"Utilities","text":"<pre><code>let a = Matrix::new([[1.0_f64, -2.0], [-3.0, 4.0]]);\n\nlet s = a.sum();        // sum of all elements = 0.0\nlet b = a.abs();        // element-wise abs \u2192 [[1,2],[3,4]]\nlet c = a.map(|x| x * x);  // element-wise map\n\n// Row/column swap\nlet mut m = Matrix3::&lt;f64&gt;::eye();\nm.swap_rows(0, 2);\nm.swap_cols(0, 1);\n\n// As flat slice (column-major order)\nlet s: &amp;[f64] = a.as_slice();\n</code></pre>"},{"location":"matrix/#iteration","title":"Iteration","text":"<pre><code>let a = Matrix::new([[1.0_f64, 2.0], [3.0, 4.0]]);\n\nfor val in a.iter() {\n    // visits in column-major order: 1.0, 3.0, 2.0, 4.0\n}\n\nfor val in a {    // IntoIterator, consumes a\n    // same order\n}\n</code></pre>"},{"location":"matrix/#linear-algebra","title":"Linear Algebra","text":"<p>Linear algebra operations are documented on the Linalg page. Convenience methods available directly on <code>Matrix</code>:</p> <pre><code>let a = Matrix::new([[4.0_f64, 2.0], [2.0, 3.0]]);\n\nlet lu   = a.lu().unwrap();\nlet chol = a.cholesky().unwrap();\nlet qr   = a.qr().unwrap();\nlet svd  = a.svd().unwrap();\nlet eig  = a.eig_symmetric().unwrap();\nlet sch  = a.schur().unwrap();\n\nlet inv  = a.inverse().unwrap();\nlet det  = a.det();\nlet (re, im) = a.eigenvalues().unwrap();\n</code></pre>"},{"location":"no-std/","title":"No-std / Embedded","text":"<p>numeris is designed to run on embedded targets with no operating system, no heap, and no floating-point hardware. All core modules compile and run correctly in <code>no_std</code> environments.</p>"},{"location":"no-std/#quick-start-for-embedded","title":"Quick Start for Embedded","text":"<pre><code># Cargo.toml\n[dependencies]\nnumeris = { version = \"0.2\", default-features = false, features = [\"libm\"] }\n</code></pre> <p>This gives you:</p> <ul> <li>Fixed-size <code>Matrix&lt;T, M, N&gt;</code> (stack-allocated, no heap)</li> <li><code>Quaternion&lt;T&gt;</code> rotations</li> <li>All linear algebra decompositions (LU, Cholesky, QR, SVD, Eigen, Schur)</li> <li>ODE integration: RK4 (<code>rk4</code>, <code>rk4_step</code> \u2014 fully no-alloc)</li> <li>Special functions (with <code>special</code> feature)</li> </ul>"},{"location":"no-std/#feature-flag-guide","title":"Feature Flag Guide","text":"Goal <code>features</code> <code>default-features</code> Full desktop use <code>[\"all\"]</code> or default yes No-std, stack-only <code>[\"libm\"]</code> no No-std + heap <code>[\"libm\", \"alloc\"]</code> no No-std + ODE <code>[\"libm\", \"ode\"]</code> no No-std + filters <code>[\"libm\", \"control\"]</code> no No-std + EKF only <code>[\"libm\", \"estimate\"]</code> \u2014 but estimate implies alloc no <p>Alloc without std</p> <p>The <code>alloc</code> feature enables <code>DynMatrix</code>, <code>DynVector</code>, and the sigma-point filters (UKF, SR-UKF, CKF). It works on targets with a global allocator but no full <code>std</code>. On bare-metal embedded, you need to provide your own allocator via <code>#[global_allocator]</code>.</p>"},{"location":"no-std/#float-math","title":"Float Math","text":"Mode Float source Performance <code>std</code> enabled System libm (hardware FPU) Hardware speed <code>no_std</code> + <code>libm</code> Pure-Rust <code>libm</code> crate Software emulation <p>numeris selects float math via the <code>num-traits</code> <code>Float</code> trait. When <code>std</code> is enabled, this delegates to the system's hardware-backed <code>sin</code>, <code>sqrt</code>, etc. Without <code>std</code>, it uses the <code>libm</code> crate's software implementations \u2014 accurate to within 1 ULP on all platforms.</p>"},{"location":"no-std/#modules-available-without-heap","title":"Modules Available Without Heap","text":"<p>These modules work with <code>#![no_std]</code> and zero heap allocation:</p> Module No-heap Notes <code>matrix</code> \u2713 Stack-allocated <code>Matrix&lt;T, M, N&gt;</code> <code>quaternion</code> \u2713 <code>linalg</code> \u2713 All six decompositions <code>ode</code> \u2713 (RK4) Adaptive solvers need <code>alloc</code> for dense output <code>control</code> \u2713 Filters and PID <code>special</code> \u2713 <code>stats</code> \u2713 <code>interp</code> \u2713 Fixed-size variants (<code>LinearInterp&lt;T, N&gt;</code>, etc.) <code>optim</code> \u2713 Root finding, BFGS, GN, LM <code>estimate</code> \u2713 (EKF, BatchLsq) UKF/SR-UKF/CKF require <code>alloc</code> <code>dynmatrix</code> \u2717 Requires <code>alloc</code>"},{"location":"no-std/#example-embedded-ekf-no-heap","title":"Example: Embedded EKF (no heap)","text":"<pre><code>#![no_std]\n#![no_main]\n\nuse numeris::estimate::Ekf;\nuse numeris::{ColumnVector, Matrix};\n\n// 4-state IMU attitude EKF (quaternion + bias)\n// Works on Cortex-M4/M7 with FPU, no heap required\nstatic mut EKF: Option&lt;Ekf&lt;f32, 4, 3&gt;&gt; = None;\n\npub fn init() {\n    let x0 = ColumnVector::&lt;f32, 4&gt;::zeros();\n    let p0 = Matrix::&lt;f32, 4, 4&gt;::eye();\n\n    unsafe { EKF = Some(Ekf::new(x0, p0)); }\n}\n\npub fn imu_update(gyro: [f32; 3], dt: f32) {\n    let q  = Matrix::&lt;f32, 4, 4&gt;::eye();  // process noise\n    let r  = Matrix::&lt;f32, 3, 3&gt;::eye();  // measurement noise\n\n    unsafe {\n        if let Some(ref mut ekf) = EKF {\n            ekf.predict(\n                |x| {\n                    // Quaternion kinematics: q_dot = 0.5 * Omega(gyro - bias) * q\n                    ColumnVector::zeros()  // simplified\n                },\n                |_x| Matrix::eye(),\n                Some(&amp;q),\n            );\n        }\n    }\n}\n</code></pre>"},{"location":"no-std/#example-fixed-step-rk4-no-heap-no-alloc","title":"Example: Fixed-Step RK4 (no heap, no alloc)","text":"<pre><code>#![no_std]\nuse numeris::ode::rk4_step;\nuse numeris::Vector;\n\n// Satellite orbital mechanics \u2014 called from interrupt\npub fn propagate(t: f64, y: &amp;Vector&lt;f64, 6&gt;, dt: f64) -&gt; Vector&lt;f64, 6&gt; {\n    rk4_step(t, y, |_t, state| {\n        // Two-body gravity: f = [v; -\u03bc/r\u00b3 * r]\n        let r = state.head::&lt;3&gt;();\n        let v = state.tail::&lt;3&gt;();\n        let r3 = r.norm().powi(3);\n        let mu = 3.986e14_f64;\n\n        let mut deriv = Vector::zeros();\n        for i in 0..3 { deriv[i] = v[i]; }\n        for i in 0..3 { deriv[i+3] = -mu / r3 * r[i]; }\n        deriv\n    }, dt)\n}\n</code></pre>"},{"location":"no-std/#target-considerations","title":"Target Considerations","text":"Target class Heap FPU SIMD Recommended config Cortex-M0/M0+ no no no <code>no-default-features</code>, <code>libm</code> Cortex-M4F/M7 optional yes no <code>no-default-features</code>, <code>libm</code> + FPU target RISC-V (bare-metal) optional optional no <code>no-default-features</code>, <code>libm</code> aarch64 (Linux) yes yes NEON default or <code>all</code> x86_64 (Linux/macOS) yes yes SSE2/AVX default or <code>all</code>"},{"location":"no-std/#cortex-m-with-fpu","title":"Cortex-M with FPU","text":"<p>On Cortex-M4F/M7 with hardware FPU, you can get near-hardware-speed float math by using the target's native float ABI:</p> <pre><code>cargo build \\\n  --target thumbv7em-none-eabihf \\\n  --no-default-features \\\n  --features \"libm,ode,estimate\" \\\n  --release\n</code></pre> <p>The <code>thumbv7em-none-eabihf</code> target uses the VFP/FPU instructions natively. numeris's <code>f32</code> and <code>f64</code> operations will use hardware float through <code>libm</code>'s software path on this target class \u2014 for best performance, you may want to configure a linker override that routes <code>libm</code> calls to the Cortex-M CMSIS-DSP library.</p>"},{"location":"no-std/#integer-matrices","title":"Integer Matrices","text":"<p>Integer <code>Matrix&lt;i32, M, N&gt;</code>, <code>Matrix&lt;u64, M, N&gt;</code>, etc. work with any scalar type implementing the <code>Scalar</code> trait (no float required). Only float-specific operations (det, norms, decompositions) require <code>FloatScalar</code> or <code>LinalgScalar</code>.</p> <pre><code>#![no_std]\nuse numeris::Matrix;\n\nlet a = Matrix::&lt;i32, 2, 2&gt;::new([[1, 2], [3, 4]]);\nlet b = Matrix::&lt;i32, 2, 2&gt;::new([[5, 6], [7, 8]]);\nlet c = a * b;   // integer matrix multiply \u2014 no float, no alloc\n</code></pre>"},{"location":"ode/","title":"ODE Integration","text":"<p>numeris provides fixed-step and adaptive ODE integrators for non-stiff and stiff systems. All solvers work on <code>Vector&lt;T, N&gt;</code> state vectors and closure-based dynamics functions.</p> <p>Requires the <code>ode</code> feature (default).</p>"},{"location":"ode/#fixed-step-rk4","title":"Fixed-Step RK4","text":"<p>Classic 4th-order Runge-Kutta \u2014 no error estimation, fixed step size.</p> <pre><code>use numeris::ode::{rk4, rk4_step};\nuse numeris::Vector;\n\nlet f = |_t: f64, y: &amp;Vector&lt;f64, 2&gt;| {\n    Vector::from_array([y[1], -y[0]])  // harmonic oscillator\n};\n\nlet y0 = Vector::from_array([1.0_f64, 0.0]);\nlet dt = 0.01;\nlet t_end = 2.0 * std::f64::consts::PI;\n\n// Integrate from t=0 to t=t_end\nlet sol = rk4(0.0, t_end, &amp;y0, f, dt);\n\n// Or take a single step\nlet y1 = rk4_step(0.0, &amp;y0, f, dt);\n</code></pre> <p>RK4 is fully no-std and no-alloc \u2014 suitable for embedded real-time control loops.</p>"},{"location":"ode/#adaptive-solvers","title":"Adaptive Solvers","text":"<p>Seven adaptive Runge-Kutta solvers via the <code>RKAdaptive</code> trait. All use embedded error estimation and a PI step-size controller (S\u00f6derlind &amp; Wang 2006) for automatic step adjustment.</p> <pre><code>use numeris::ode::{RKAdaptive, RKTS54, AdaptiveSettings};\nuse numeris::Vector;\n\nlet y0 = Vector::from_array([1.0_f64, 0.0]);\nlet tau = 2.0 * std::f64::consts::PI;\n\nlet sol = RKTS54::integrate(\n    0.0, tau, &amp;y0,\n    |_t, y| Vector::from_array([y[1], -y[0]]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\n\nprintln!(\"y(2\u03c0) = {:?}\", sol.y);       // final state\nprintln!(\"steps taken = {}\", sol.steps); // number of accepted steps\n</code></pre>"},{"location":"ode/#solver-table","title":"Solver Table","text":"Solver Stages Order FSAL Interpolant Best for <code>RKF45</code> 6 5(4) no \u2014 Classic baseline <code>RKTS54</code> 7 5(4) yes 4th degree General purpose <code>RKV65</code> 10 6(5) no 6th degree Moderate accuracy <code>RKV87</code> 17 8(7) no 7th degree High accuracy <code>RKV98</code> 21 9(8) no 8th degree Very high accuracy <code>RKV98NoInterp</code> 16 9(8) no \u2014 Very high acc., no dense output <code>RKV98Efficient</code> 26 9(8) no 9th degree Max accuracy + dense output <p>FSAL (First Same As Last): the last function evaluation of one step is reused as the first of the next, saving one function evaluation per step.</p>"},{"location":"ode/#adaptivesettings","title":"AdaptiveSettings","text":"<pre><code>use numeris::ode::AdaptiveSettings;\n\nlet settings = AdaptiveSettings {\n    rtol: 1e-8,       // relative tolerance (default 1e-6)\n    atol: 1e-10,      // absolute tolerance (default 1e-9)\n    h0:   Some(0.01), // initial step size (auto if None)\n    hmax: Some(1.0),  // maximum step size (unlimited if None)\n    max_steps: 100_000, // step limit\n    ..AdaptiveSettings::default()\n};\n</code></pre>"},{"location":"ode/#solution-struct","title":"Solution struct","text":"<pre><code>let sol = RKTS54::integrate(0.0, 1.0, &amp;y0, f, &amp;settings).unwrap();\n\nlet y_final = &amp;sol.y;       // final state Vector&lt;T, N&gt;\nlet t_final =  sol.t;       // final time (= t_end if successful)\nlet n_steps  = sol.steps;   // number of accepted steps\nlet n_reject = sol.rejected; // number of rejected steps\n</code></pre>"},{"location":"ode/#dense-output-interpolation","title":"Dense Output (Interpolation)","text":"<p>Solvers with an interpolant can return intermediate values at arbitrary times without re-integrating. Requires <code>std</code> feature (uses <code>Vec</code> internally).</p> <pre><code>use numeris::ode::{RKAdaptive, RKTS54, AdaptiveSettings};\nuse numeris::Vector;\n\nlet y0 = Vector::from_array([1.0_f64, 0.0]);\n\nlet mut sol = RKTS54::integrate_dense(\n    0.0, 6.28, &amp;y0,\n    |_t, y| Vector::from_array([y[1], -y[0]]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\n\n// Interpolate at arbitrary time point\nlet y_at_pi = sol.interpolate(std::f64::consts::PI);\n// y[0] \u2248 -1.0 (cosine at \u03c0)\n</code></pre>"},{"location":"ode/#stiff-solver-rodas4","title":"Stiff Solver: RODAS4","text":"<p>For stiff ODEs (chemical kinetics, circuit simulation, orbital mechanics with drag), use <code>RODAS4</code> \u2014 an L-stable linearly-implicit Rosenbrock method. It solves linear systems involving the Jacobian instead of nonlinear Newton iterations, making it suitable for very stiff problems without tuning.</p> <pre><code>use numeris::ode::{Rosenbrock, RODAS4, AdaptiveSettings};\nuse numeris::{Vector, Matrix};\n\n// Stiff decay: y' = -1000y,  y(0) = 1\nlet y0 = Vector::from_array([1.0_f64]);\n\n// With user-supplied Jacobian\nlet sol = RODAS4::integrate(\n    0.0, 0.01, &amp;y0,\n    |_t, y| Vector::from_array([-1000.0 * y[0]]),\n    |_t, _y| Matrix::new([[-1000.0_f64]]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\nassert!((sol.y[0] - (-10.0_f64).exp()).abs() &lt; 1e-8);\n\n// With automatic finite-difference Jacobian (no need to supply \u2202f/\u2202y)\nlet sol2 = RODAS4::integrate_auto(\n    0.0, 0.01, &amp;y0,\n    |_t, y| Vector::from_array([-1000.0 * y[0]]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\n</code></pre>"},{"location":"ode/#van-der-pol-oscillator-1000","title":"Van der Pol oscillator (\u03bc = 1000)","text":"<pre><code>use numeris::ode::{Rosenbrock, RODAS4, AdaptiveSettings};\nuse numeris::{Vector, Matrix};\n\nlet mu = 1000.0_f64;\nlet y0 = Vector::from_array([2.0_f64, 0.0]);\n\nlet sol = RODAS4::integrate(\n    0.0, 3000.0, &amp;y0,\n    |_t, y| Vector::from_array([y[1], mu * (1.0 - y[0] * y[0]) * y[1] - y[0]]),\n    |_t, y| Matrix::new([\n        [0.0,                                    1.0],\n        [-2.0 * mu * y[0] * y[1] - 1.0, mu * (1.0 - y[0] * y[0])],\n    ]),\n    &amp;AdaptiveSettings::default(),\n).unwrap();\n</code></pre>"},{"location":"ode/#rodas4-properties","title":"RODAS4 Properties","text":"Property Value Stages 6 Order 4(3) embedded pair L-stable Yes (no oscillation for arbitrarily large step sizes) Stiffly accurate Yes Jacobian User-supplied or auto finite-difference"},{"location":"ode/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::ode::OdeError;\n\nmatch RKTS54::integrate(0.0, 1.0, &amp;y0, f, &amp;settings) {\n    Ok(sol) =&gt; { /* success */ }\n    Err(OdeError::MaxStepsExceeded) =&gt; { /* increase max_steps or rtol/atol */ }\n    Err(OdeError::StepSizeTooSmall) =&gt; { /* likely a stiff problem \u2014 use RODAS4 */ }\n    Err(OdeError::SingularJacobian) =&gt; { /* RODAS4 specific */ }\n}\n</code></pre>"},{"location":"optim/","title":"Optimization","text":"<p>Root finding, unconstrained minimization, and nonlinear least squares.</p> <p>Requires the <code>optim</code> Cargo feature:</p> <pre><code>numeris = { version = \"0.2\", features = [\"optim\"] }\n</code></pre>"},{"location":"optim/#algorithm-summary","title":"Algorithm Summary","text":"Algorithm Function Use case Brent's method <code>brent</code> Bracketed scalar root finding \u2014 superlinear convergence Newton 1D <code>newton_1d</code> Scalar root finding with analytic derivative BFGS <code>minimize_bfgs</code> Unconstrained smooth minimization Gauss-Newton <code>least_squares_gn</code> Nonlinear least squares (QR-based, full-rank Jacobian) Levenberg-Marquardt <code>least_squares_lm</code> Nonlinear least squares (damped, more robust)"},{"location":"optim/#root-finding","title":"Root Finding","text":""},{"location":"optim/#brents-method","title":"Brent's Method","text":"<p>Bracketed root finding \u2014 guaranteed to converge given a sign change.</p> <pre><code>use numeris::optim::{brent, RootSettings};\n\n// Solve x\u00b2 - 2 = 0 on [0, 2]\nlet result = brent(\n    |x| x * x - 2.0,\n    0.0_f64, 2.0,\n    &amp;RootSettings::default(),\n).unwrap();\n\nassert!((result.x - std::f64::consts::SQRT_2).abs() &lt; 1e-12);\nprintln!(\"root = {}, f(root) = {}, iters = {}\", result.x, result.fx, result.iterations);\n</code></pre>"},{"location":"optim/#newton-1d","title":"Newton 1D","text":"<p>Scalar Newton's method with analytic derivative \u2014 quadratic convergence near the root.</p> <pre><code>use numeris::optim::{newton_1d, RootSettings};\n\n// Solve cos(x) = 0 near x = 1.5\nlet result = newton_1d(\n    |x| x.cos(),\n    |x| -x.sin(),\n    1.5_f64,\n    &amp;RootSettings::default(),\n).unwrap();\n\nassert!((result.x - std::f64::consts::FRAC_PI_2).abs() &lt; 1e-12);\n</code></pre>"},{"location":"optim/#settings","title":"Settings","text":"<pre><code>use numeris::optim::RootSettings;\n\nlet settings = RootSettings {\n    tol:       1e-12,   // convergence tolerance on |f(x)| and bracket width\n    max_iter:  200,\n    ..RootSettings::default()\n};\n</code></pre>"},{"location":"optim/#bfgs-minimization","title":"BFGS Minimization","text":"<p>Quasi-Newton unconstrained minimization with Armijo backtracking line search. Requires both a function and its gradient.</p> <pre><code>use numeris::optim::{minimize_bfgs, BfgsSettings};\nuse numeris::Vector;\n\n// Minimize the Rosenbrock function: (1-x)\u00b2 + 100(y-x\u00b2)\u00b2\nlet result = minimize_bfgs(\n    |v: &amp;Vector&lt;f64, 2&gt;| {\n        let x = v[0]; let y = v[1];\n        (1.0 - x).powi(2) + 100.0 * (y - x * x).powi(2)\n    },\n    |v: &amp;Vector&lt;f64, 2&gt;| {\n        let x = v[0]; let y = v[1];\n        Vector::from_array([\n            -2.0 * (1.0 - x) - 400.0 * x * (y - x * x),\n             200.0 * (y - x * x),\n        ])\n    },\n    &amp;Vector::from_array([-1.0, 1.0]),   // initial guess\n    &amp;BfgsSettings::default(),\n).unwrap();\n\nassert!((result.x[0] - 1.0).abs() &lt; 1e-5);\nassert!((result.x[1] - 1.0).abs() &lt; 1e-5);\n</code></pre>"},{"location":"optim/#using-finite-difference-gradient","title":"Using Finite-Difference Gradient","text":"<p>When an analytic gradient is unavailable:</p> <pre><code>use numeris::optim::{minimize_bfgs, finite_difference_gradient, BfgsSettings};\nuse numeris::Vector;\n\nlet f = |v: &amp;Vector&lt;f64, 2&gt;| (v[0] - 1.0).powi(2) + (v[1] - 2.0).powi(2);\nlet grad = |v: &amp;Vector&lt;f64, 2&gt;| finite_difference_gradient(f, v);\n\nlet result = minimize_bfgs(f, grad, &amp;Vector::from_array([0.0, 0.0]), &amp;BfgsSettings::default()).unwrap();\n</code></pre>"},{"location":"optim/#bfgssettings","title":"BfgsSettings","text":"<pre><code>use numeris::optim::BfgsSettings;\n\nlet settings = BfgsSettings {\n    grad_tol:  1e-8,    // gradient norm convergence criterion\n    max_iter:  1000,\n    ..BfgsSettings::default()\n};\n</code></pre>"},{"location":"optim/#gauss-newton-least-squares","title":"Gauss-Newton Least Squares","text":"<p>QR-based Gauss-Newton for nonlinear least squares. Works best when the Jacobian is full rank and the residual is small at the solution.</p> <pre><code>use numeris::optim::{least_squares_gn, GnSettings};\nuse numeris::{Matrix, Vector};\n\n// Fit y = a * exp(b * x) to noisy data\nlet t = [0.0_f64, 1.0, 2.0, 3.0, 4.0];\nlet y = [2.0_f64, 2.7, 3.65, 4.95, 6.7];\n\nlet result = least_squares_gn(\n    // Residual function: r_i = model(x) - y_i\n    |x: &amp;Vector&lt;f64, 2&gt;| {\n        let mut r = Vector::&lt;f64, 5&gt;::zeros();\n        for i in 0..5 { r[i] = x[0] * (x[1] * t[i]).exp() - y[i]; }\n        r\n    },\n    // Jacobian \u2202r/\u2202x\n    |x: &amp;Vector&lt;f64, 2&gt;| {\n        let mut j = Matrix::&lt;f64, 5, 2&gt;::zeros();\n        for i in 0..5 {\n            let e = (x[1] * t[i]).exp();\n            j[(i, 0)] = e;\n            j[(i, 1)] = x[0] * t[i] * e;\n        }\n        j\n    },\n    &amp;Vector::from_array([1.0, 0.5]),    // initial guess [a, b]\n    &amp;GnSettings::default(),\n).unwrap();\n\nprintln!(\"a = {:.4}, b = {:.4}\", result.x[0], result.x[1]);\nprintln!(\"cost = {:.6}\", result.cost);\n</code></pre>"},{"location":"optim/#levenberg-marquardt","title":"Levenberg-Marquardt","text":"<p>Damped Gauss-Newton with adaptive regularization \u2014 more robust than pure GN when the Jacobian is ill-conditioned or the initial guess is far from the solution.</p> <pre><code>use numeris::optim::{least_squares_lm, LmSettings};\nuse numeris::{Matrix, Vector};\n\nlet t = [0.0_f64, 1.0, 2.0, 3.0, 4.0];\nlet y = [2.0_f64, 2.7, 3.65, 4.95, 6.7];\n\nlet result = least_squares_lm(\n    |x: &amp;Vector&lt;f64, 2&gt;| {\n        let mut r = Vector::&lt;f64, 5&gt;::zeros();\n        for i in 0..5 { r[i] = x[0] * (x[1] * t[i]).exp() - y[i]; }\n        r\n    },\n    |x: &amp;Vector&lt;f64, 2&gt;| {\n        let mut j = Matrix::&lt;f64, 5, 2&gt;::zeros();\n        for i in 0..5 {\n            let e = (x[1] * t[i]).exp();\n            j[(i, 0)] = e;\n            j[(i, 1)] = x[0] * t[i] * e;\n        }\n        j\n    },\n    &amp;Vector::from_array([1.0, 0.1]),\n    &amp;LmSettings::default(),\n).unwrap();\n\nassert!(result.cost &lt; 0.1);\n</code></pre>"},{"location":"optim/#lmsettings","title":"LmSettings","text":"<pre><code>use numeris::optim::LmSettings;\n\nlet settings = LmSettings {\n    grad_tol:  1e-8,      // \u2225J\u1d40r\u2225 convergence criterion\n    step_tol:  1e-8,      // step size convergence criterion\n    cost_tol:  1e-8,      // cost reduction convergence criterion\n    lambda0:   1e-3,      // initial damping factor\n    max_iter:  200,\n    ..LmSettings::default()\n};\n</code></pre>"},{"location":"optim/#finite-difference-utilities","title":"Finite Difference Utilities","text":"<pre><code>use numeris::optim::{finite_difference_gradient, finite_difference_jacobian};\nuse numeris::{Matrix, Vector};\n\n// Gradient of a scalar function \u211d\u207f \u2192 \u211d\nlet f = |x: &amp;Vector&lt;f64, 3&gt;| x[0].powi(2) + x[1].powi(2) + x[2].powi(2);\nlet x = Vector::from_array([1.0_f64, 2.0, 3.0]);\nlet g: Vector&lt;f64, 3&gt; = finite_difference_gradient(f, &amp;x);\n// g \u2248 [2, 4, 6]\n\n// Jacobian of a vector function \u211d\u207f \u2192 \u211d\u1d50  (M residuals, N parameters)\nlet r = |x: &amp;Vector&lt;f64, 2&gt;| Vector::from_array([x[0].powi(2) - 1.0, x[1].powi(2) - 4.0]);\nlet x2 = Vector::from_array([1.0_f64, 2.0]);\nlet j: Matrix&lt;f64, 2, 2&gt; = finite_difference_jacobian(r, &amp;x2);\n</code></pre>"},{"location":"optim/#result-types","title":"Result Types","text":"<pre><code>use numeris::optim::{OptimResult, LsqResult};\n\n// Scalar root finding\n// result.x         \u2014 solution scalar\n// result.fx        \u2014 f(x) at solution\n// result.iterations \u2014 iterations taken\n\n// BFGS minimization\n// result.x         \u2014 solution Vector&lt;T, N&gt;\n// result.fx        \u2014 f(x) at solution\n// result.grad_norm \u2014 \u2225\u2207f\u2225 at solution\n// result.iterations \u2014 iterations taken\n\n// Least squares\n// result.x         \u2014 solution Vector&lt;T, N&gt;\n// result.cost      \u2014 \u00bd\u2016r\u2016\u00b2 at solution\n// result.iterations \u2014 iterations taken\n</code></pre>"},{"location":"optim/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::optim::OptimError;\n\nmatch least_squares_lm(r, j, &amp;x0, &amp;settings) {\n    Ok(result)                          =&gt; { /* converged */ }\n    Err(OptimError::MaxIterExceeded)    =&gt; { /* increase max_iter or relax tolerances */ }\n    Err(OptimError::LineSearchFailed)   =&gt; { /* BFGS line search failure */ }\n    Err(OptimError::SingularJacobian)   =&gt; { /* Gauss-Newton needs better initial guess */ }\n    Err(e) =&gt; { /* other */ }\n}\n</code></pre>"},{"location":"performance/","title":"Performance","text":"<p>numeris achieves competitive performance via SIMD intrinsics, register-blocked micro-kernels, and direct formulas for small sizes.</p>"},{"location":"performance/#simd-architecture","title":"SIMD Architecture","text":"<p>SIMD is always-on for <code>f32</code> and <code>f64</code> \u2014 no feature flag, no runtime detection. Dispatch selects the widest available ISA at compile time via <code>#[cfg(target_feature)]</code>. Integer and complex types fall back to scalar loops via <code>TypeId</code> dispatch, with zero runtime overhead (dead-code eliminated at monomorphization).</p> Architecture ISA f64 tile (MR\u00d7NR) f32 tile (MR\u00d7NR) aarch64 NEON (128-bit) 8\u00d74 8\u00d74 x86_64 SSE2 (128-bit) 4\u00d74 8\u00d74 x86_64 AVX (256-bit) 8\u00d74 16\u00d74 x86_64 AVX-512 (512-bit) 16\u00d74 32\u00d74 other scalar fallback 4\u00d74 4\u00d74 <p>AVX and AVX-512 require compile-time opt-in:</p> <pre><code>RUSTFLAGS=\"-C target-cpu=native\" cargo build --release\n# or explicitly:\nRUSTFLAGS=\"-C target-feature=+avx2,+avx512f\" cargo build --release\n</code></pre> <p>SSE2 (x86_64) and NEON (aarch64) are always-on baselines.</p>"},{"location":"performance/#matrix-multiply-micro-kernels","title":"Matrix Multiply Micro-Kernels","text":"<p>The matmul micro-kernels are inspired by nano-gemm and faer (Sarah Quinones). Each kernel:</p> <ol> <li>Tiles the output matrix into MR\u00d7NR panels</li> <li>Loads A and B into SIMD registers</li> <li>Accumulates the full k-sum into MR\u00d7NR SIMD accumulators (never reading/writing C in the inner loop)</li> <li>Writes the tile to C exactly once</li> </ol> <p>This reduces memory traffic by O(n) compared to a naive C-read-per-k implementation. The k-loop is cache-blocked at KC=256 elements for L1 cache locality.</p> <pre><code>For each (block_i, block_j) tile of C:\n  accum[0..MR*NR] = 0\n  for k = 0..K:\n    load A[block_i..block_i+MR, k] into MR SIMD vectors\n    load B[k, block_j..block_j+NR] into NR scalars\n    for row in 0..MR: accum[row*NR..] += A_row * B_col\n  store accum \u2192 C[block_i..block_i+MR, block_j..block_j+NR]\n</code></pre>"},{"location":"performance/#optimizations-applied","title":"Optimizations Applied","text":"<ul> <li>Direct formulas for N \u2264 4: <code>inverse()</code> and <code>det()</code> use adjugate-based closed-form expressions, bypassing LU decomposition entirely.</li> <li>Unrolled LU/Cholesky for N \u2264 6: direct <code>data[col][row]</code> array indexing eliminates trait dispatch in inner loops.</li> <li>AXPY SIMD: LU elimination, QR Householder, SVD bidiagonalization, and Hessenberg reduction all use <code>axpy_neg_dispatch</code> on contiguous column slices.</li> <li>4-accumulator dot product: reduces dependency chains in the dot product inner loop.</li> </ul>"},{"location":"performance/#benchmark-results","title":"Benchmark Results","text":"<p>Platform: Apple Silicon M-series (aarch64 NEON), Rust stable.</p> <p>Compared against nalgebra 0.33 and faer 0.19. All benchmarks run with <code>cargo bench</code>.</p> Benchmark numeris nalgebra faer Winner matmul 4\u00d74 7.1 ns 5.2 ns 61 ns nalgebra matmul 6\u00d76 21.2 ns 21.1 ns 93 ns ~tie matmul 50\u00d750 (dyn) 6.8 \u00b5s 6.9 \u00b5s 6.5 \u00b5s ~tie matmul 200\u00d7200 (dyn) 367 \u00b5s 310 \u00b5s 174 \u00b5s faer dot 100 (dyn) 12.4 ns 13.2 ns \u2014 numeris LU 4\u00d74 34.8 ns 30.2 ns 212.5 ns nalgebra LU 6\u00d76 90.3 ns 80.8 ns 300.9 ns nalgebra LU 50\u00d750 (dyn) 8.9 \u00b5s 7.8 \u00b5s 8.0 \u00b5s nalgebra Cholesky 4\u00d74 26.5 ns 12.2 ns 142.1 ns nalgebra Cholesky 6\u00d76 76.5 ns 42.1 ns 202.9 ns nalgebra QR 4\u00d74 62.9 ns 101.7 ns 328.4 ns numeris QR 6\u00d76 85.9 ns 217.3 ns 521.5 ns numeris SVD 4\u00d74 313.9 ns 489.0 ns 1365.7 ns numeris SVD 6\u00d76 1135.5 ns 975.3 ns 2011.1 ns nalgebra Inverse 4\u00d74 29.6 ns 24.8 ns \u2014 nalgebra Eigen sym 4\u00d74 183.7 ns 213.4 ns 621.5 ns numeris Eigen sym 6\u00d76 352.0 ns 574.8 ns 1200.5 ns numeris"},{"location":"performance/#summary","title":"Summary","text":"<ul> <li>numeris wins: QR (2.5\u00d7), SVD 4\u00d74, symmetric eigendecomposition (1.5\u20132\u00d7), dot product, matmul 50\u00d750</li> <li>nalgebra wins: small matmul 4\u00d74, Cholesky, LU, inverse \u2014 gaps are small</li> <li>faer wins: large dynamic matmul (200\u00d7200) \u2014 A/B panel packing dominates at that scale</li> <li>faer has high overhead at small sizes due to dynamic dispatch / runtime machinery</li> </ul>"},{"location":"performance/#improvements-vs-initial-baseline","title":"Improvements vs. Initial Baseline","text":"Benchmark Before After Speedup matmul 200\u00d7200 562 \u00b5s 367 \u00b5s 1.5\u00d7 matmul 50\u00d750 12.5 \u00b5s 6.8 \u00b5s 1.8\u00d7 matmul 6\u00d76 62.1 ns 21.2 ns 2.9\u00d7 inverse 4\u00d74 79.9 ns 29.6 ns 2.7\u00d7 LU 4\u00d74 45.3 ns 34.8 ns 1.3\u00d7 dot 100 25.2 ns 12.4 ns 2.0\u00d7"},{"location":"performance/#remaining-opportunities","title":"Remaining Opportunities","text":"<ul> <li>Cholesky: still ~2\u00d7 behind nalgebra \u2014 bottleneck is sqrt/division cost, not dispatch overhead</li> <li>Large matmul: A/B panel packing could close the remaining ~2\u00d7 gap with faer</li> <li>Small matmul 4\u00d74: nalgebra's hardcoded unrolled kernel is difficult to beat generically</li> </ul>"},{"location":"performance/#no-std-performance","title":"No-std Performance","text":"<p>On embedded targets with no hardware FPU, float operations fall back to the <code>libm</code> software implementation. Performance in this mode is entirely determined by the target's ALU throughput \u2014 SIMD code paths are not compiled for targets without SIMD (the <code>TypeId</code> dispatch compiles down to scalar loops).</p>"},{"location":"quaternion/","title":"Quaternion","text":"<p>Unit quaternion 3D rotations with scalar-first convention <code>[w, x, y, z]</code>.</p> <p><code>Quaternion&lt;T&gt;</code> is available in the crate root with no feature flag.</p>"},{"location":"quaternion/#convention","title":"Convention","text":"<p>numeris uses the scalar-first (Hamilton) convention:</p> <pre><code>q = w + xi + yj + zk    where w = cos(\u03b8/2), [x,y,z] = sin(\u03b8/2)\u00b7axis\n</code></pre> <ul> <li><code>q * p</code> composes rotation <code>q</code> applied after rotation <code>p</code></li> <li><code>q * v</code> rotates vector <code>v</code> by the rotation represented by <code>q</code></li> </ul>"},{"location":"quaternion/#construction","title":"Construction","text":"<pre><code>use numeris::{Quaternion, Vector};\n\n// Identity (no rotation)\nlet id = Quaternion::&lt;f64&gt;::identity();\n\n// From axis-angle\nlet q = Quaternion::from_axis_angle(\n    &amp;Vector::from_array([0.0_f64, 0.0, 1.0]),  // z-axis\n    std::f64::consts::FRAC_PI_2,               // 90\u00b0\n);\n\n// From Euler angles (roll, pitch, yaw in radians \u2014 ZYX convention)\nlet q_euler = Quaternion::from_euler(0.0_f64, 0.0, std::f64::consts::FRAC_PI_4);\n\n// From rotation matrix (3\u00d73 orthogonal)\nuse numeris::Matrix3;\nlet rot = Matrix3::&lt;f64&gt;::eye();  // identity rotation\nlet q_mat = Quaternion::from_rotation_matrix(&amp;rot);\n\n// Elementary rotations\nlet qx = Quaternion::rotx(std::f64::consts::PI / 6.0); // 30\u00b0 around x\nlet qy = Quaternion::roty(std::f64::consts::FRAC_PI_4); // 45\u00b0 around y\nlet qz = Quaternion::rotz(std::f64::consts::FRAC_PI_2); // 90\u00b0 around z\n\n// Direct construction (auto-normalized)\nlet q_raw = Quaternion::new(1.0_f64, 0.0, 0.0, 0.0);  // identity\n</code></pre>"},{"location":"quaternion/#vector-rotation","title":"Vector Rotation","text":"<pre><code>use numeris::{Quaternion, Vector};\n\n// 90\u00b0 rotation around z-axis\nlet q = Quaternion::from_axis_angle(\n    &amp;Vector::from_array([0.0_f64, 0.0, 1.0]),\n    std::f64::consts::FRAC_PI_2,\n);\n\nlet v = Vector::from_array([1.0_f64, 0.0, 0.0]);\nlet rotated = q * v;   // \u2248 [0, 1, 0]\n\nassert!((rotated[0] - 0.0).abs() &lt; 1e-14);\nassert!((rotated[1] - 1.0).abs() &lt; 1e-14);\nassert!((rotated[2] - 0.0).abs() &lt; 1e-14);\n</code></pre>"},{"location":"quaternion/#composition","title":"Composition","text":"<pre><code>use numeris::{Quaternion, Vector};\n\nlet q1 = Quaternion::rotx(std::f64::consts::FRAC_PI_2);  // 90\u00b0 x\nlet q2 = Quaternion::rotz(std::f64::consts::FRAC_PI_2);  // 90\u00b0 z\n\n// Apply q1 first, then q2\nlet combined = q2 * q1;\n\n// Equivalently:\nlet v = Vector::from_array([1.0_f64, 0.0, 0.0]);\nlet r1 = q2 * (q1 * v);   // step by step\nlet r2 = combined * v;     // combined rotation\n// r1 \u2248 r2\n</code></pre>"},{"location":"quaternion/#inverse-and-conjugate","title":"Inverse and Conjugate","text":"<p>For unit quaternions, conjugate = inverse:</p> <pre><code>let q = Quaternion::from_axis_angle(\n    &amp;Vector::from_array([0.0_f64, 0.0, 1.0]),\n    1.0,\n);\n\nlet q_conj = q.conjugate();   // q* = [w, -x, -y, -z]\nlet q_inv  = q.inverse();     // same as conjugate for unit quaternions\n\n// q * q^{-1} = identity\nlet id = q * q_inv;\nassert!((id.w() - 1.0).abs() &lt; 1e-14);\n</code></pre>"},{"location":"quaternion/#interpolation-slerp","title":"Interpolation (SLERP)","text":"<p>Spherical linear interpolation \u2014 constant angular velocity, smooth path on SO(3).</p> <pre><code>use numeris::Quaternion;\n\nlet q0 = Quaternion::&lt;f64&gt;::identity();\nlet q1 = Quaternion::rotz(std::f64::consts::FRAC_PI_2);\n\n// t=0 \u2192 q0, t=1 \u2192 q1, t=0.5 \u2192 halfway (45\u00b0)\nlet q_half = q0.slerp(&amp;q1, 0.5);\n\n// Use for smooth animation or attitude interpolation\nfor i in 0..=10 {\n    let t = i as f64 / 10.0;\n    let q = q0.slerp(&amp;q1, t);\n    // q represents i*9\u00b0 rotation around z-axis\n}\n</code></pre>"},{"location":"quaternion/#conversion","title":"Conversion","text":"<pre><code>use numeris::Quaternion;\n\nlet q = Quaternion::from_axis_angle(\n    &amp;numeris::Vector::from_array([0.0_f64, 0.0, 1.0]),\n    1.2,\n);\n\n// To rotation matrix (3\u00d73 orthogonal)\nlet rot = q.to_rotation_matrix();   // Matrix3&lt;f64&gt;\n\n// To axis-angle (axis is unit vector, angle in radians)\nlet (axis, angle) = q.to_axis_angle();\n\n// To Euler angles (ZYX: roll, pitch, yaw)\nlet (roll, pitch, yaw) = q.to_euler();\n\n// Components\nlet w = q.w();\nlet x = q.x();\nlet y = q.y();\nlet z = q.z();\n\n// Normalize (in case of accumulated numerical drift)\nlet q_norm = q.normalize();\n</code></pre>"},{"location":"quaternion/#operations","title":"Operations","text":"<pre><code>let q = Quaternion::rotz(1.0_f64);\n\n// Hamilton product (composition)\nlet q2 = q * q;         // 2 radians around z\n\n// Scalar operations\nlet q_scaled = q * 2.0; // NOT a unit quaternion \u2014 use normalize() after\n\n// Norm (should be \u2248 1.0 for properly constructed quaternions)\nlet n = q.norm();\nassert!((n - 1.0).abs() &lt; 1e-14);\n</code></pre>"},{"location":"quaternion/#attitude-determination-example","title":"Attitude Determination Example","text":"<pre><code>use numeris::{Quaternion, Vector};\n\n// Represent spacecraft attitude as quaternion (body frame relative to ECI)\nlet q_body_to_eci = Quaternion::from_euler(\n    0.1_f64,   // roll  10\u00b0\n    -0.05,     // pitch -5\u00b0\n    1.57,      // yaw   90\u00b0\n);\n\n// Transform a vector from body to ECI frame\nlet boresight_body = Vector::from_array([1.0_f64, 0.0, 0.0]);\nlet boresight_eci  = q_body_to_eci * boresight_body;\n\n// Attitude error between two frames\nlet q_target = Quaternion::identity();\nlet q_error  = q_target * q_body_to_eci.inverse();\nlet (axis, angle) = q_error.to_axis_angle();\n// |angle| is the pointing error magnitude\n</code></pre>"},{"location":"special/","title":"Special Functions","text":"<p>Mathematical special functions used throughout probability and statistics.</p> <p>Requires the <code>special</code> Cargo feature:</p> <pre><code>numeris = { version = \"0.2\", features = [\"special\"] }\n</code></pre> <p>All functions work with both <code>f32</code> and <code>f64</code>, are no-std compatible, and have no heap allocation.</p>"},{"location":"special/#gamma-function","title":"Gamma Function","text":"<p>The gamma function <code>\u0393(x)</code> generalizes the factorial: <code>\u0393(n) = (n-1)!</code> for positive integers.</p> <p>Implemented via the Lanczos approximation (g=7, n=9 coefficients) with the reflection formula for negative arguments. Exact factorial lookup table for integer arguments 1\u201321.</p> <pre><code>use numeris::special::{gamma, lgamma};\n\n// Gamma function\nlet g = gamma(5.0_f64);          // 4! = 24.0\nlet g_half = gamma(0.5_f64);     // \u221a\u03c0 \u2248 1.7724538509\nlet g_neg = gamma(-0.5_f64);     // -2\u221a\u03c0 \u2248 -3.5449077018\n\n// Log-gamma: more numerically stable for large arguments\nlet lg = lgamma(100.0_f64);      // log(99!) \u2248 359.13\nlet lg2 = lgamma(0.5_f64);       // log(\u221a\u03c0) \u2248 0.5724\n\n// Poles: gamma(0), gamma(-1), gamma(-2), ... \u2192 \u00b1\u221e\nassert!(gamma(0.0_f64).is_infinite());\n</code></pre>"},{"location":"special/#digamma-function","title":"Digamma Function","text":"<p>The digamma function <code>\u03c8(x) = d/dx ln \u0393(x)</code> \u2014 the logarithmic derivative of the gamma function.</p> <p>Implemented via recurrence (shifts argument to x \u2265 6) + 7-term asymptotic expansion. Reflection formula for negative arguments.</p> <pre><code>use numeris::special::digamma;\n\nlet psi1 = digamma(1.0_f64);   // -\u03b3 \u2248 -0.5772156649 (Euler-Mascheroni constant)\nlet psi2 = digamma(2.0_f64);   //  1 - \u03b3 \u2248 0.4227843351\nlet psi_neg = digamma(-0.5_f64); // \u03c8(-1/2) \u2248 0.03648997...\n\n// Poles at non-positive integers \u2192 NaN\nassert!(digamma(0.0_f64).is_nan());\n</code></pre>"},{"location":"special/#beta-function","title":"Beta Function","text":"<p><code>B(a, b) = \u0393(a)\u0393(b) / \u0393(a+b)</code>. Implemented via <code>lgamma</code> delegation.</p> <pre><code>use numeris::special::{beta, lbeta};\n\nlet b = beta(2.0_f64, 3.0_f64);   // 1/12 \u2248 0.08333...\nlet lb = lbeta(2.0_f64, 3.0_f64); // log(1/12) \u2248 -2.4849...\n\n// B(1/2, 1/2) = \u03c0 (related to the arc-sine distribution)\nlet b_half = beta(0.5_f64, 0.5_f64); // \u2248 3.14159...\n</code></pre>"},{"location":"special/#regularized-incomplete-gamma","title":"Regularized Incomplete Gamma","text":"<p><code>P(a, x) = \u03b3(a, x) / \u0393(a)</code> (lower) and <code>Q(a, x) = \u0393(a, x) / \u0393(a)</code> (upper).</p> <ul> <li>Series expansion for <code>x &lt; a + 1</code> (fast convergence near zero)</li> <li>Lentz continued fraction for <code>x \u2265 a + 1</code> (fast convergence in the tail)</li> <li>Maximum 200 iterations; returns <code>SpecialError::ConvergenceFailure</code> if not converged</li> </ul> <pre><code>use numeris::special::{gamma_inc, gamma_inc_upper};\n\n// P(a, x): probability that a Gamma(a,1) random variable \u2264 x\nlet p = gamma_inc(2.0_f64, 1.0_f64);          // \u2248 0.2642\nlet p_full = gamma_inc(2.0_f64, f64::INFINITY); // 1.0\n\n// Q(a, x) = 1 - P(a, x)\nlet q = gamma_inc_upper(2.0_f64, 1.0_f64);    // \u2248 0.7358\n\n// Domain: a &gt; 0, x \u2265 0\n</code></pre>"},{"location":"special/#regularized-incomplete-beta","title":"Regularized Incomplete Beta","text":"<p><code>I_x(a, b)</code>: regularized incomplete beta function. Used in the CDFs of the Beta, F, and Student's t distributions.</p> <p>Implemented via Lentz continued fraction with symmetry relation <code>I_x(a,b) = 1 - I_{1-x}(b,a)</code>.</p> <pre><code>use numeris::special::betainc;\n\n// I_x(a, b): CDF of Beta(a, b) at x\nlet i = betainc(0.5_f64, 2.0_f64, 3.0_f64);  // I_{0.5}(2, 3) \u2248 0.6875\n\n// Boundary cases\nassert_eq!(betainc(0.0_f64, 2.0_f64, 3.0_f64), 0.0);\nassert_eq!(betainc(1.0_f64, 2.0_f64, 3.0_f64), 1.0);\n</code></pre>"},{"location":"special/#error-function","title":"Error Function","text":"<p><code>erf(x)</code> and <code>erfc(x) = 1 - erf(x)</code>, implemented via the regularized incomplete gamma function for numerical stability:</p> <pre><code>erf(x)  = sign(x) \u00b7 P(1/2, x\u00b2)\nerfc(x) = Q(1/2, x\u00b2)   for x \u2265 0\n</code></pre> <p>Using <code>erfc</code> for large positive <code>x</code> avoids catastrophic cancellation in <code>1 - erf(x)</code>.</p> <pre><code>use numeris::special::{erf, erfc};\n\nlet e  = erf(1.0_f64);         // \u2248 0.8427007929\nlet ec = erfc(1.0_f64);        // \u2248 0.1572992071\nassert!((e + ec - 1.0).abs() &lt; 1e-15);\n\n// Symmetry\nassert_eq!(erf(-1.0_f64), -erf(1.0_f64));\n\n// Tails\nlet e3 = erfc(3.0_f64);       // \u2248 2.21e-5 (complement is more accurate than 1-erf)\nlet e6 = erfc(6.0_f64);       // \u2248 2.15e-17\n</code></pre>"},{"location":"special/#float-type-support","title":"Float Type Support","text":"<p>All functions work with both <code>f32</code> and <code>f64</code>:</p> <pre><code>use numeris::special::{gamma, erf};\n\nlet g32 = gamma(5.0_f32);     // f32 result\nlet e64 = erf(1.0_f64);       // f64 result\n</code></pre>"},{"location":"special/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::special::SpecialError;\n\nmatch gamma_inc(-1.0_f64, 1.0_f64) {  // a must be &gt; 0\n    Err(SpecialError::DomainError)        =&gt; { /* invalid argument */ }\n    Err(SpecialError::ConvergenceFailure) =&gt; { /* series/CF didn't converge (200 iterations) */ }\n    Ok(p) =&gt; { /* success */ }\n}\n</code></pre>"},{"location":"special/#function-reference","title":"Function Reference","text":"Function Signature Notes <code>gamma(x)</code> <code>T \u2192 T</code> \u0393(x); poles at 0, -1, -2, \u2026 \u2192 \u00b1\u221e <code>lgamma(x)</code> <code>T \u2192 T</code> ln|\u0393(x)|; use for large arguments <code>digamma(x)</code> <code>T \u2192 T</code> \u03c8(x) = d/dx ln \u0393(x); poles \u2192 NaN <code>beta(a, b)</code> <code>T, T \u2192 T</code> B(a,b) = \u0393(a)\u0393(b)/\u0393(a+b) <code>lbeta(a, b)</code> <code>T, T \u2192 T</code> ln B(a,b); more stable <code>gamma_inc(a, x)</code> <code>T, T \u2192 Result&lt;T&gt;</code> Regularized lower incomplete gamma P(a,x) <code>gamma_inc_upper(a, x)</code> <code>T, T \u2192 Result&lt;T&gt;</code> Regularized upper incomplete gamma Q(a,x) <code>betainc(x, a, b)</code> <code>T, T, T \u2192 T</code> Regularized incomplete beta I_x(a,b) <code>erf(x)</code> <code>T \u2192 T</code> Error function <code>erfc(x)</code> <code>T \u2192 T</code> Complementary error function 1 - erf(x)"},{"location":"stats/","title":"Statistics","text":"<p>Ten probability distributions, each implementing <code>ContinuousDistribution&lt;T&gt;</code> or <code>DiscreteDistribution&lt;T&gt;</code> traits.</p> <p>Requires the <code>stats</code> Cargo feature (implies <code>special</code>):</p> <pre><code>numeris = { version = \"0.2\", features = [\"stats\"] }\n</code></pre> <p>All distributions work with <code>f32</code> and <code>f64</code>, are no-std compatible, and have no heap allocation.</p>"},{"location":"stats/#distribution-traits","title":"Distribution Traits","text":"<pre><code>pub trait ContinuousDistribution&lt;T&gt; {\n    fn pdf(&amp;self, x: T) -&gt; T;       // probability density function\n    fn cdf(&amp;self, x: T) -&gt; T;       // cumulative distribution function\n    fn mean(&amp;self) -&gt; T;\n    fn variance(&amp;self) -&gt; T;\n    fn std_dev(&amp;self) -&gt; T;\n}\n\npub trait DiscreteDistribution&lt;T&gt; {\n    fn pmf(&amp;self, k: u64) -&gt; T;     // probability mass function P(X = k)\n    fn cdf(&amp;self, k: u64) -&gt; T;     // cumulative P(X \u2264 k)\n    fn mean(&amp;self) -&gt; T;\n    fn variance(&amp;self) -&gt; T;\n    fn std_dev(&amp;self) -&gt; T;\n}\n</code></pre>"},{"location":"stats/#continuous-distributions","title":"Continuous Distributions","text":""},{"location":"stats/#normal-gaussian","title":"Normal (Gaussian)","text":"<p><code>Normal&lt;T&gt; { mean: T, std_dev: T }</code></p> <pre><code>use numeris::stats::Normal;\n\nlet n = Normal::new(0.0_f64, 1.0).unwrap();  // standard normal N(0,1)\nlet p = n.pdf(0.0);      // 1/\u221a(2\u03c0) \u2248 0.3989\nlet c = n.cdf(1.645);    // \u2248 0.95 (95th percentile)\n\nlet n2 = Normal::new(100.0_f64, 15.0).unwrap(); // IQ distribution\nlet prob_above_130 = 1.0 - n2.cdf(130.0);       // P(IQ &gt; 130) \u2248 2.3%\n</code></pre>"},{"location":"stats/#uniform","title":"Uniform","text":"<p><code>Uniform&lt;T&gt; { a: T, b: T }</code> \u2014 continuous uniform on [a, b].</p> <pre><code>use numeris::stats::Uniform;\n\nlet u = Uniform::new(0.0_f64, 1.0).unwrap();\nlet p = u.pdf(0.5);      // 1.0 (constant density)\nlet c = u.cdf(0.3);      // 0.3\nassert_eq!(u.mean(), 0.5);\nassert!((u.variance() - 1.0/12.0).abs() &lt; 1e-10);\n</code></pre>"},{"location":"stats/#exponential","title":"Exponential","text":"<p><code>Exponential&lt;T&gt; { rate: T }</code> \u2014 rate \u03bb (mean = 1/\u03bb).</p> <pre><code>use numeris::stats::Exponential;\n\nlet e = Exponential::new(2.0_f64).unwrap();  // \u03bb=2, mean=0.5\nlet p = e.pdf(1.0);      // 2\u00b7e\u207b\u00b2 \u2248 0.2707\nlet c = e.cdf(1.0);      // 1 - e\u207b\u00b2 \u2248 0.8647\nassert!((e.mean() - 0.5).abs() &lt; 1e-12);\n</code></pre>"},{"location":"stats/#gamma","title":"Gamma","text":"<p><code>Gamma&lt;T&gt; { shape: T, rate: T }</code> \u2014 shape \u03b1, rate \u03b2 (mean = \u03b1/\u03b2).</p> <p>The chi-squared and exponential distributions are special cases.</p> <pre><code>use numeris::stats::Gamma;\n\nlet g = Gamma::new(2.0_f64, 1.0).unwrap();  // Gamma(\u03b1=2, \u03b2=1)\nlet p = g.pdf(1.0);   // 1\u00b7e\u207b\u00b9 \u2248 0.3679\nlet c = g.cdf(2.0);   // uses regularized incomplete gamma P(2, 2)\nassert!((g.mean() - 2.0).abs() &lt; 1e-10);\nassert!((g.variance() - 2.0).abs() &lt; 1e-10);\n</code></pre>"},{"location":"stats/#beta","title":"Beta","text":"<p><code>Beta&lt;T&gt; { alpha: T, beta: T }</code> \u2014 shape parameters \u03b1, \u03b2. Support: [0, 1].</p> <pre><code>use numeris::stats::Beta;\n\nlet b = Beta::new(2.0_f64, 5.0).unwrap();\nlet p = b.pdf(0.3);    // uses regularized incomplete beta\nlet c = b.cdf(0.5);\nassert!((b.mean() - 2.0/7.0).abs() &lt; 1e-10);\n</code></pre>"},{"location":"stats/#chi-squared","title":"Chi-Squared","text":"<p><code>ChiSquared&lt;T&gt; { k: T }</code> \u2014 k degrees of freedom. Special case of Gamma(k/2, 1/2).</p> <pre><code>use numeris::stats::ChiSquared;\n\nlet chi = ChiSquared::new(3.0_f64).unwrap();  // \u03c7\u00b2(3)\nlet p_val = 1.0 - chi.cdf(7.815);    // p-value for \u03c7\u00b2=7.815 with 3 df \u2248 0.05\nassert_eq!(chi.mean(), 3.0);\nassert_eq!(chi.variance(), 6.0);\n</code></pre>"},{"location":"stats/#students-t","title":"Student's t","text":"<p><code>StudentT&lt;T&gt; { nu: T }</code> \u2014 \u03bd degrees of freedom.</p> <pre><code>use numeris::stats::StudentT;\n\nlet t = StudentT::new(10.0_f64).unwrap();  // t(10)\nlet c = t.cdf(2.228);   // \u2248 0.975 (95% two-sided CI critical value)\nlet p = t.pdf(0.0);     // peak of distribution\nassert_eq!(t.mean(), 0.0);\nassert!((t.variance() - 10.0/8.0).abs() &lt; 1e-10);  // \u03bd/(\u03bd-2)\n</code></pre>"},{"location":"stats/#discrete-distributions","title":"Discrete Distributions","text":""},{"location":"stats/#bernoulli","title":"Bernoulli","text":"<p><code>Bernoulli&lt;T&gt; { p: T }</code> \u2014 single trial with success probability p.</p> <pre><code>use numeris::stats::Bernoulli;\n\nlet b = Bernoulli::new(0.3_f64).unwrap();\nassert!((b.pmf(1) - 0.3).abs() &lt; 1e-12);\nassert!((b.pmf(0) - 0.7).abs() &lt; 1e-12);\nassert!((b.mean() - 0.3).abs() &lt; 1e-12);\nassert!((b.variance() - 0.21).abs() &lt; 1e-12);\n</code></pre>"},{"location":"stats/#binomial","title":"Binomial","text":"<p><code>Binomial&lt;T&gt; { n: u64, p: T }</code> \u2014 n independent Bernoulli trials.</p> <pre><code>use numeris::stats::Binomial;\n\nlet b = Binomial::new(10, 0.5_f64).unwrap();    // B(10, 0.5)\nlet p5 = b.pmf(5);    // P(X=5) \u2248 0.2461\nlet c7  = b.cdf(7);   // P(X\u22647) \u2248 0.9453\nassert!((b.mean() - 5.0).abs() &lt; 1e-10);\nassert!((b.variance() - 2.5).abs() &lt; 1e-10);\n</code></pre>"},{"location":"stats/#poisson","title":"Poisson","text":"<p><code>Poisson&lt;T&gt; { lambda: T }</code> \u2014 number of events in a fixed interval.</p> <pre><code>use numeris::stats::Poisson;\n\nlet pois = Poisson::new(3.0_f64).unwrap();  // Poisson(\u03bb=3)\nlet p3 = pois.pmf(3);    // P(X=3) = e\u207b\u00b3\u00b73\u00b3/3! \u2248 0.2240\nlet c5 = pois.cdf(5);    // P(X\u22645) \u2248 0.9161\nassert_eq!(pois.mean(), 3.0);\nassert_eq!(pois.variance(), 3.0);\n</code></pre>"},{"location":"stats/#distribution-summary-table","title":"Distribution Summary Table","text":"Distribution Type Parameters Mean Variance <code>Normal</code> Continuous \u03bc, \u03c3 \u03bc \u03c3\u00b2 <code>Uniform</code> Continuous a, b (a+b)/2 (b-a)\u00b2/12 <code>Exponential</code> Continuous \u03bb 1/\u03bb 1/\u03bb\u00b2 <code>Gamma</code> Continuous \u03b1, \u03b2 \u03b1/\u03b2 \u03b1/\u03b2\u00b2 <code>Beta</code> Continuous \u03b1, \u03b2 \u03b1/(\u03b1+\u03b2) \u03b1\u03b2/((\u03b1+\u03b2)\u00b2(\u03b1+\u03b2+1)) <code>ChiSquared</code> Continuous k k 2k <code>StudentT</code> Continuous \u03bd 0 (\u03bd&gt;1) \u03bd/(\u03bd-2) (\u03bd&gt;2) <code>Bernoulli</code> Discrete p p p(1-p) <code>Binomial</code> Discrete n, p np np(1-p) <code>Poisson</code> Discrete \u03bb \u03bb \u03bb"},{"location":"stats/#error-handling","title":"Error Handling","text":"<pre><code>use numeris::stats::StatsError;\n\nmatch Normal::new(0.0_f64, -1.0) {  // std_dev must be &gt; 0\n    Err(StatsError::InvalidParameter) =&gt; { /* bad parameter */ }\n    Ok(n) =&gt; { /* use n */ }\n}\n</code></pre>"}]}